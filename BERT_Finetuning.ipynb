{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fstTqT2KYIoK"
   },
   "source": [
    "Where BERT is Still Used Today\n",
    "\n",
    "1. Search & Retrieval (Vector Search, RAG Base Models)\n",
    "\n",
    "- For generating dense embeddings (e.g., Sentence-Transformers, MiniLM).\n",
    "\n",
    "- Stored in FAISS, Pinecone, Milvus for fast similarity search.\n",
    "\n",
    "- Used in smaller LLM pipelines (retriever + generator architecture).\n",
    "\n",
    "2. Enterprise-level NLP Tasks (Fast & Cost-Effective)\n",
    "\n",
    "- Named Entity Recognition (NER)\n",
    "\n",
    "- Sentiment Analysis\n",
    "\n",
    "- Classification tasks (spam detection, intent classification)\n",
    "\n",
    "- Summarization using lightweight variants (DistilBERT).\n",
    "\n",
    "3. Hybrid Pipelines with LLMs\n",
    "\n",
    "- BERT embeddings for the retriever, then an LLM generates the answer (RAG architecture).\n",
    "\n",
    "4. Multilingual NLP\n",
    "\n",
    "- XLM-R (a multilingual BERT version) is still a top choice for 100+ languages.\n",
    "\n",
    "- Used for translation and cross-lingual search.\n",
    "\n",
    "5. On-Device / Low-Latency Inference\n",
    "\n",
    "- For mobile apps and edge devices where GPT/Claude can’t run.\n",
    "\n",
    "- Quantized DistilBERT/MiniLM models for chatbots and offline NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWc1nGddZExy"
   },
   "source": [
    "| **Model**           | **Main Use-Cases**                                  | **Why Still Used**                    |\n",
    "| ------------------- | --------------------------------------------------- | ------------------------------------- |\n",
    "| BERT / DistilBERT   | NER, classification, embeddings                     | Small, fast, cheap inference          |\n",
    "| RoBERTa / DeBERTa   | Classification, QA, summarization                   | High accuracy, Kaggle/enterprise use  |\n",
    "| MPNet / MiniLM      | Vector search, semantic retrieval (RAG)             | Best for FAISS/Pinecone retrieval     |\n",
    "| T5 / Flan-T5        | Summarization, translation, instruction tasks       | Lightweight text-to-text generation   |\n",
    "| BART / Pegasus      | Abstractive summarization                           | Less resource-hungry than LLMs        |\n",
    "| XLNet / Electra     | Classification, QA (legacy setups)                  | Still optimized for speed             |\n",
    "| XLM-R / mT5 / LaBSE | Multilingual NLP, translation, cross-lingual search | 100+ language support, enterprise use |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sD6ApJatYMS8",
    "outputId": "fa74e957-43df-4516-db37-736e792aa000"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade datasets fsspec transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j46yH4p_zZmo"
   },
   "source": [
    "# text-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gfUAIalozeSq"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SZQEPvczeVs"
   },
   "outputs": [],
   "source": [
    "# take the below complex dataset\n",
    "# load_dataset(\"ag_news\")\n",
    "# load_dataset(\"dbpedia_14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vm34eDTkzeYK"
   },
   "outputs": [],
   "source": [
    "# Customer feedback classification (positive/negative/neutral)\n",
    "# Support ticket intent detection (billing, technical, general)\n",
    "# Email/topic categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "8132c0699db843129fa3db5b6a9a9d84",
      "2f259433eec3443084e309574f7cb687",
      "6b11d46bacf348f9b8b6135d81e0ef62",
      "3f423ee439a74e30b1aa5d76a5fb2b4d",
      "b35710cdcf4248ffa729b04887026cb6",
      "40412a11abab4aa7890761eaa2972937",
      "c3785e98994a4d4e9190a0745c11a8b5",
      "895abad4ce96469eb5fbebc13eb21604",
      "ea5e7750dce94f48a941976325ac0655",
      "f5d399904e0f49b1a345ee96066c9885",
      "38033995e8494e8a91aa849f773d410f",
      "88e7c8bc82794b858148f3ce87233b53",
      "bf0773174a3848769efe386194d0aa5a",
      "2fc5ed8932e64b7a8800f2f51c99779a",
      "dcaa298eb4114512b48ae078b0296329",
      "2c07129ad48343aeaad13f95dca6b6eb",
      "c3a615dffb7b4f2893dd48233cac977e",
      "2b649a13854f46cdba78744b9a08ef33",
      "ed2167c8ed3846439fd71fca2acf0bae",
      "4fb5e0275b4e419cb9f4ac69960d55fe",
      "29cdb909a11548a8a3511735592b2735",
      "e98ec7dee665400cbda567ecbe52578c",
      "d7391c7f11044e1bb0e1013b2d69c595",
      "c3f1a1aad3db4f7a95b7ac710a23bf88",
      "79dafb63ee734dfaa95dbbfb9dc6a4d5",
      "9d9af211930c42e6aae6441d2fc59b36",
      "10936d30901b4d6092dc82a7dc99e4ca",
      "3e6631cf7174411c975076fbe17a6057",
      "6ee2d5fd5a3d412882faf7cb3f665d22",
      "8b9437286da14c4396f469b9c5c04531",
      "360a8ce71467443094b0f1fbcd0e4d85",
      "5f3d123b85444b969ed2544bc615527c",
      "b113ee1c140542e296c664d79e8a2f64",
      "9afc2b48076043e781a13e6397b5f6e9",
      "675d4aaa45724a30806ee2c5a14279f7",
      "72aafd0084cb4c43b8a5a94e80e22bd7",
      "42468e7b35d64ab0b5d7b23aa1c92c23",
      "6f881b824c704b03850500cd34f1bef6",
      "7754b3c342ed432b91354e4e1e52f307",
      "5324576472214c79a16ca87bfc39f00c",
      "88aa9d88293e46b783002730e15dca7a",
      "07b2b19bec2342589dff49062250306e",
      "dc47b78df0bb4ebc91900e261edfcf7a",
      "478cdf954b754850b684fccd56cf29aa",
      "dfb37bacea964e128792ab01bb8d5657",
      "4ca70806bf834b189c7b283b43b6d760",
      "c0351b84db70466ba02dd838cba0c8e0",
      "4ce8e755e54d4344b5e946be96bd9035",
      "75eb72285aee45bc850cd8ad6810ff49",
      "1760a2933b8244a6bd3673109b7f1fba",
      "91bbdf475af14a378e6eae581392013f",
      "2a95d24126ab4a239bd222fcbd744579",
      "91574689901e428e852186875745bfc0",
      "f1486e3ee4d14fc0bc9b14f41d96e3fd",
      "9670992aca8749aa9254a5061d217cc3",
      "43491cb9863e4d2f9ee516e3b5cd0583",
      "eebdb7319c5f47e0a4ab10ecf0d9ff10",
      "c4ba3763c1e14e8cab8cdcbf116249bc",
      "cfb3f69581f946dcb15401d38dcf9581",
      "e758c5cb2ae9444eb2a56eecc13a5215",
      "179a700945d3465fa7027c02822db892",
      "6a1a0d77d9a34067b789980de4d6a089",
      "84edc45a7910423e9adb070f4518ae51",
      "8e19718377364a01a1a9425c17ce83ca",
      "b757c1b9ea2a4dcfbf5bf639533540b8",
      "1d7875ca5f8e4ebaac85faab21ce0d55",
      "f341263dd8cc4a2394f7497cdf18990a",
      "1e9ed994c04d477f85a8f27de1827ab1",
      "05829f7a39394259bb5ef413f11d2794",
      "32f46bccf8af40a5b32f18d40a24e35b",
      "b25af2089c17465db01a365664e8b768",
      "a9765ceb350b4d66abf690c12ef3486f",
      "6b608a558b734d4d93d2e2f37daf9111",
      "47b5fbf9a54e48fbb84bb729e3359dfc",
      "6ef0ab4ffce74f619e26ceb552194830",
      "d2ed7545af8a48afb6f832858f140414",
      "1ff5073d617042508bfb01801e87803c"
     ]
    },
    "id": "7imbEzlkzecj",
    "outputId": "b4e238b1-0a8f-4a3e-ac7b-868c115f9b0a"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "# Load IMDB dataset and subset\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jUPG3wQtLx2E",
    "outputId": "231c0f85-5a1f-4459-db2b-7c38047d564c"
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJkLvHt1zee9"
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"].select(range(1000))\n",
    "test_dataset = dataset[\"test\"].select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dkEOxqA2z21_"
   },
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkLNandqV-UD"
   },
   "source": [
    "\n",
    "\n",
    "##### The raw text column (Each row in the IMDB dataset contains a review text)\n",
    "##### It will pad each sentence to the same length (up to 256 tokens)\n",
    "##### If the text is longer than 256 tokens, it will truncate (cut) it\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijD42dn2z24i"
   },
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YaRZULDgz27F"
   },
   "outputs": [],
   "source": [
    "# Apply tokenization + rename + format in a single flow\n",
    "def preprocess(ds):\n",
    "    ds = ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])  # remove raw text (saves memory)\n",
    "    ds = ds.rename_column(\"label\", \"labels\")\n",
    "    ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "b9a0bf2b685c456094b6c68c58dd548c",
      "f6484d63485841629b61f5c5322de600",
      "c69535ecf5b44a0793aa89076b1dad92",
      "ac199b4953b94ad9a49e7824ffd7d7db",
      "8158c78b8f364a11aa81cfde354adcd5",
      "d88fd75783ea4f1aaf140c82ab7d0109",
      "f7abcb7aa80643d5bbf34929f091b984",
      "eb00ee1512744800a53dd51f0cdb6bf6",
      "fe2f4970245049b9a115439e3ce62215",
      "f598e869df6043b397d666edef84b83b",
      "21e9fd984ada4cb7919d9bb8456ad3a9"
     ]
    },
    "id": "-eBrtHUcz29W",
    "outputId": "c41fcdbb-c075-4965-993f-f77a0e71f99d"
   },
   "outputs": [],
   "source": [
    "train_dataset = preprocess(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "df5f5e08bae14d359a4d8dfa8818dedd",
      "12eb8666f24c491c940eb91f3ed7704e",
      "470f347202ea4b9f9a481e63136a50d8",
      "e1fab53d117a41a990bc2a218884a456",
      "3647125eb3c44af2930596d8234fcb6f",
      "d8e86dca159a4bc8bc53d8958d73636c",
      "6fef6a77c9e74bf09d4a555ce95f69c6",
      "896aec85c08548d29846fe7ae6fb8760",
      "edea57d9bf7342cca920a9788ccec426",
      "4f32bd0d6be44f3985aa6d91bb9b6e72",
      "346a937a5ab4492db890c43eea343f74"
     ]
    },
    "id": "vFKiKAIiz-GC",
    "outputId": "630fcf34-fc24-41e1-a315-587265d6705a"
   },
   "outputs": [],
   "source": [
    "test_dataset = preprocess(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105,
     "referenced_widgets": [
      "a20a46e945034fbdad9a656e42b14cef",
      "7643f8fdf24b4b1da31927015b7f5b5c",
      "c0d71f4ae4c34e86a7484985a201d485",
      "a5041c6bce6541be881c86e7aa9aa654",
      "450961029e444a779af85584954a7f5b",
      "fc40288439884d6c8ac92a83c344288b",
      "9f5e8d5425cc4f77b608439d14cad739",
      "806b6218761b462383901f85bd7bfa1b",
      "6ec92b211316487b957f3de96e36f629",
      "d9135f99ac074e3cb91893319bcfc8b4",
      "39b271c5507d418fa2ef8264383f8c36"
     ]
    },
    "id": "ERFLeSvwz-8P",
    "outputId": "72f06084-d945-4eaa-ddfe-4f3d8ee8f184"
   },
   "outputs": [],
   "source": [
    "# 3. Model load\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ml-Y00kfRxNj",
    "outputId": "ea565cf3-6710-4e99-820a-42413f47aefc"
   },
   "outputs": [],
   "source": [
    "for layer in model.bert.encoder.layer:\n",
    "  print(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tldMs-8z--1"
   },
   "outputs": [],
   "source": [
    "### Finetune only Last few layers  and other layer freeze.\n",
    "## Classifier head trainable by default\n",
    "\n",
    "# for param in model.bert.parameters():\n",
    "#     param.requires_grad = False  # Freeze BERT encoder\n",
    "\n",
    "## Unfreeze last 2 encoder layers\n",
    "\n",
    "# for layer in model.bert.encoder.layer[-2:]:\n",
    "#     for param in layer.parameters():\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JH89dQ2FsI8B"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-finetuned-imdb\",       # Directory to save model checkpoints and outputs\n",
    "    num_train_epochs=1,                       # Number of training epochs (1 full pass over the training data)\n",
    "    per_device_train_batch_size=8,            # Batch size per GPU/CPU device\n",
    "    logging_dir=\"./logs\",                     # Directory to store logs for visualization (e.g., with TensorBoard)\n",
    "    learning_rate=2e-5,                       # Learning rate for the AdamW optimizer (typically low for BERT)\n",
    "    weight_decay=0.01,                        # Weight decay (L2 regularization) to reduce overfitting\n",
    "    report_to=\"none\"                          # Disable reporting to external tracking tools (e.g., wandb, hub)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8cR8gys70Yi6"
   },
   "outputs": [],
   "source": [
    "# 5. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "id": "cQhrExLJ0bpQ",
    "outputId": "d3a778de-e9be-424c-95b7-741f98817bbb"
   },
   "outputs": [],
   "source": [
    "# 6. Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D0zDMVGrrqg-",
    "outputId": "03ba6dd9-a0f7-454a-afc0-b8b9ec483bb9"
   },
   "outputs": [],
   "source": [
    "!tensorboard --logdir=./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBmgg3Pn0dMr"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"./bert-finetuned-imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1aLbbW4x0dPV",
    "outputId": "08b8d80e-0a42-40e5-be16-7fd8e87920f3"
   },
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"./bert-finetuned-imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "c2aJKe_h0e8j",
    "outputId": "9ca9a349-ae6f-422a-b5d9-4c5856cf3431"
   },
   "outputs": [],
   "source": [
    "# 7. Evaluate\n",
    "metrics = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Y3Wug-n0fAk",
    "outputId": "07f574f5-0ddd-4ad1-8ba8-91ed8228617d"
   },
   "outputs": [],
   "source": [
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmJq0Nwz0hs3"
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MsQZuD860jps"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"/content/bert-finetuned-imdb\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"/content/bert-finetuned-imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N5UF9Zzh0jsN",
    "outputId": "9417774d-08e1-4f04-b481-f32cdc250024"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k-5cIQ3a0mjp"
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "text = \"This movie was amazing and I loved the acting!\"\n",
    "result = classifier(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n1Q9PAln0mmC",
    "outputId": "d271d901-4afb-4e30-d018-011818ec7085"
   },
   "outputs": [],
   "source": [
    "print(result)  # Example: [{'label': 'POSITIVE', 'score': 0.98}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdev1wUG0q2h"
   },
   "source": [
    "### Pushing it to Huggingfacehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "0bcad6973f1d4c1998efc2ec965eade0",
      "72d30933ca7246098be7d85b4b18a1c1"
     ]
    },
    "id": "kLeh1DNI0qOA",
    "outputId": "aeb9a665-00ba-4b98-f3ac-c396c5c47ad2"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0iDLXdrf0qQd",
    "outputId": "822fa86a-9dd8-4f8c-caac-edb81344010e"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import whoami\n",
    "print(whoami())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "QblwcgII0vUC",
    "outputId": "c48ac480-e69a-4ddb-e826-5f83f4e4b98b"
   },
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub(\"ganesh/my-bert-imdb2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167,
     "referenced_widgets": [
      "94f074ca5147474882a2ac4e7c87bee4",
      "c890938fdfd74401a40e19c94016a94b",
      "5876f07e1a864121a2b882721a27fac4",
      "bdc97a6e4fb64e4c94724b6016a3631f",
      "d62773dbf63642beaa70f00030ecbcc7",
      "0aede7cf12aa445996476fb4abb492f0",
      "32875aaaf9f748b8aeafe2b501db9c61",
      "9c98f7a3ea7f482cbaab7db5163c330b",
      "841edb9cbb64409b9c650b81f5c33a07",
      "205b6a57ad564602bfd39f5fc0c154bc",
      "1a6172a4f5784bebbee84bb165097420",
      "d0a551b8c51e4886851b3d88ef2d5b2f",
      "d10f35cfc4354a06a4bc288a7c8796ea",
      "e91eb3b17a3b41fab5ca76301a0aed2a",
      "f7b09ee396804cf49e6c690e099cfb9e",
      "ecd068ad899043c1ac255c5e5ffb2b57",
      "d0e3e27d38d84d9aac6b757a98568739",
      "28d22ee4e8af4e14adf5ccb87056f465",
      "11fb2a9d4c914f92ac216951a90fba0d",
      "3a9087baa4424a18a51d4903f83177a4",
      "d1791986dd844069b3f01f552a1beb4d",
      "d3f2f05a5fb94073b4e54aeff290baa8",
      "3e01ad5a584d4b149cd2cef1a690c29e",
      "df43abfc904e4629bfb9060529875f32",
      "5a6f81626a364444818dd5c5aabd6931",
      "d859a557e5724a55884ff42f371d023f",
      "6e8a4e1ded1c4da7ad042e3784dd9ef3",
      "ca2912becc2c48ff858229ec8c91b9b4",
      "5581017a112642df92212d55c33319dd",
      "d52a01f31aa44fffae5016ee8f22a8e5",
      "82ba3bb2ae7a40db837fe6b5d9bf9090",
      "914f2a183a594f528afff62ee1a087db",
      "8c746cbb14d04b5bb7ad1cfd7af6a916"
     ]
    },
    "id": "NP7qOhFE0x54",
    "outputId": "15350539-a6cb-4f5e-f631-f7a3c930231d"
   },
   "outputs": [],
   "source": [
    "trainer.push_to_hub(\"ganesh/my-bert-imdb2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Fine-Tuning on IMDB Sentiment Classification\n",
    "\n",
    "This project fine-tunes a pre-trained BERT model (`bert-base-uncased`) on the **IMDB movie reviews dataset** to perform **binary sentiment classification** (positive or negative).\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Task Type\n",
    "**Text Classification (Binary)**  \n",
    "Each IMDB review is labeled as:\n",
    "- `0`: Negative\n",
    "- `1`: Positive\n",
    "\n",
    "### Goal\n",
    "Fine-tune a general-purpose language model (BERT) to accurately classify movie reviews based on sentiment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjp2zHAwW3dN"
   },
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "# # 1. Load IMDB dataset (subset for speed)\n",
    "# dataset = load_dataset(\"imdb\")\n",
    "# train_dataset = dataset[\"train\"].select(range(1000))\n",
    "# test_dataset = dataset[\"test\"].select(range(500))\n",
    "\n",
    "# # 2. Initialize tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# # 3. Tokenization function (no fixed padding here)\n",
    "# def tokenize_fn(examples):\n",
    "#     return tokenizer(examples[\"text\"], truncation=True, max_length=256)\n",
    "\n",
    "# # 4. Preprocess dataset (map + rename + torch format)\n",
    "# def preprocess(ds):\n",
    "#     ds = ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])  # remove raw text\n",
    "#     ds = ds.rename_column(\"label\", \"labels\")\n",
    "#     ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "#     return ds\n",
    "\n",
    "# train_dataset = preprocess(train_dataset)\n",
    "# test_dataset = preprocess(test_dataset)\n",
    "\n",
    "# # 5. Initialize model\n",
    "# model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# # 6. Data collator (dynamic padding)\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# # 7. Training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./bert-finetuned-imdb\",   # where to save the model\n",
    "#     num_train_epochs=1,                   # train for 1 epoch\n",
    "#     per_device_train_batch_size=8,        # training batch size\n",
    "#     per_device_eval_batch_size=8,         # eval batch size\n",
    "#     logging_dir=\"./logs\",                 # logs for TensorBoard\n",
    "#     logging_steps=50,                     # log every 50 steps\n",
    "#     learning_rate=2e-5,                   # small LR for fine-tuning\n",
    "#     weight_decay=0.01,                    # regularization\n",
    "#     eval_steps=500,                       # run evaluation every 500 steps\n",
    "#     save_steps=500,                       # save checkpoint every 500 steps\n",
    "#     save_total_limit=1,                   # keep only the latest checkpoint\n",
    "#     report_to=\"none\"                      # disable WandB or other reporting\n",
    "# )\n",
    "\n",
    "\n",
    "# # 8. Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=test_dataset,\n",
    "#     data_collator=data_collator,  # dynamic padding here\n",
    "# )\n",
    "\n",
    "# # 9. Train\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3WGIzCWzXC7"
   },
   "source": [
    "# Finetune on multiple problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6KouthQeTBiw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertTokenizerFast,\n",
    "    BertForSequenceClassification,\n",
    "    BertForTokenClassification,\n",
    "    BertForQuestionAnswering,\n",
    "    get_linear_schedule_with_warmup ###### Gradually warms up then decays learning rate for stable BERT training.\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW #Adam Optimizer with Weight Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "isHzO0jScVsk",
    "outputId": "f3f1ff2a-74f7-4730-c0dc-48c319b3e31c"
   },
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpBWBTXxM8jF"
   },
   "source": [
    "# Explanation of Training Steps and Learning Rate Scheduling\n",
    "\n",
    "### Dataset size = 800 samples\n",
    "\n",
    "You have **800 training examples** in total.\n",
    "\n",
    "---\n",
    "\n",
    "### batch_size = 8 → 1 epoch = 800 / 8 = 100 batches\n",
    "\n",
    "- Your batch size is 8.\n",
    "- In one epoch, the model processes 800 samples in batches of 8.\n",
    "- Therefore, one epoch consists of **100 batches**.\n",
    "\n",
    "---\n",
    "\n",
    "### epochs = 3\n",
    "\n",
    "- The model will train over **3 full passes (epochs)** through the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### total_steps = 100 × 3 = 300\n",
    "\n",
    "- Total number of training steps (optimizer updates) is:\n",
    "  - Number of batches per epoch × number of epochs\n",
    "  - 100 × 3 = **300 steps**\n",
    "\n",
    "Each step corresponds to processing one batch and updating model weights.\n",
    "\n",
    "---\n",
    "\n",
    "### Warmup = 10% of total steps → 30 steps\n",
    "\n",
    "- Warmup is a phase at the start of training where the learning rate increases gradually from zero to its maximum value.\n",
    "- 10% of 300 steps is **30 warmup steps**.\n",
    "- For these first 30 steps, the learning rate rises linearly.\n",
    "\n",
    "---\n",
    "\n",
    "### Remaining 270 steps → Learning rate linearly decreases\n",
    "\n",
    "- After warmup, for the remaining **270 steps**, the learning rate decreases linearly.\n",
    "- This helps the model converge smoothly towards the end of training.\n",
    "\n",
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "- Train on 800 samples with batch size 8 → 100 batches per epoch.\n",
    "- Train for 3 epochs → total 300 steps.\n",
    "- Learning rate warms up for first 30 steps.\n",
    "- Learning rate then decays linearly for remaining 270 steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wV6nHeacP3Rb"
   },
   "source": [
    "# When to Use Hugging Face’s `.map()` + `.set_format()` vs Custom PyTorch Dataset Class\n",
    "\n",
    "- **Hugging Face’s `.map()` + `.set_format()`**  \n",
    "  Use this approach **if you already have your dataset in the Hugging Face Dataset format**.  \n",
    "  It’s easy to preprocess and convert your data using these built-in methods.\n",
    "\n",
    "- **Custom PyTorch Dataset Class**  \n",
    "  Use this when:  \n",
    "  - You **do not have your data in Hugging Face Dataset format** (for example, you just have plain Python lists like `train_texts` and `train_labels`).  \n",
    "  - You need **custom preprocessing logic** that goes beyond simple transformations (like using a special tokenizer or additional data processing steps).  \n",
    "  - You want more control over how data is accessed and transformed during training.\n",
    "\n",
    "### Example analogy with a simple class:\n",
    "\n",
    "```python\n",
    "class Basket:\n",
    "    def __init__(self, fruits):\n",
    "        self.fruits = fruits  # List of fruits\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fruits)  # Total number of fruits\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.fruits[idx]  # Get specific fruit by index\n",
    "\n",
    "basket = Basket([\"Apple\", \"Banana\", \"Mango\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0bmf-c5e3ll"
   },
   "outputs": [],
   "source": [
    "class Basket:\n",
    "    def __init__(self, fruits):\n",
    "        self.fruits = fruits  # List of fruits\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fruits)  # Kitne fruits hai total\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.fruits[idx]  # Index se specific fruit nikalna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "647kUXEBfA3_"
   },
   "outputs": [],
   "source": [
    "basket = Basket([\"Apple\", \"Banana\", \"Mango\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p7WaIbsge4f4",
    "outputId": "52a37842-8604-4e34-e55b-9c1d71bf62a3"
   },
   "outputs": [],
   "source": [
    "print(len(basket))\n",
    "print(basket[0])\n",
    "print(basket[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of the `TextClassificationDataset` Class\n",
    "\n",
    "This is a **custom PyTorch Dataset class** designed for text classification tasks.\n",
    "\n",
    "### Purpose:\n",
    "- To **prepare and serve text data** in a format that a PyTorch model can easily use during training or evaluation.\n",
    "- It **tokenizes the text inputs on-the-fly** using a tokenizer (like from Hugging Face’s Transformers library).\n",
    "- It converts raw texts and labels into tensors that can be fed into a neural network.\n",
    "\n",
    "---\n",
    "\n",
    "### Why use this class?\n",
    "\n",
    "- It abstracts data loading and preprocessing, so you can plug it into a PyTorch `DataLoader`.\n",
    "- Enables efficient batch processing and GPU compatibility.\n",
    "- Supports custom tokenization logic per item.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "\n",
    "This class wraps raw text and labels into a format suitable for training a text classification model with PyTorch and Hugging Face tokenizers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBhaZgqPTDWm"
   },
   "outputs": [],
   "source": [
    "# Dataset class for text classification using PyTorch\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        # texts: list of raw text samples\n",
    "        # labels: list of integer labels corresponding to texts\n",
    "        # tokenizer: a tokenizer object to convert text to token IDs\n",
    "        # max_length: max number of tokens per text (for padding/truncation)\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns the total number of samples in the dataset\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Fetches the text and label at index `idx`\n",
    "        text = str(self.texts[idx])   # Convert to string in case input isn't already\n",
    "        label = int(self.labels[idx]) # Convert label to integer\n",
    "\n",
    "        # Tokenize the text using the tokenizer\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,           # Cut off tokens beyond max_length\n",
    "            padding='max_length',      # Pad shorter texts to max_length\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'        # Return PyTorch tensors\n",
    "        )\n",
    "\n",
    "        # Return a dictionary containing:\n",
    "        # - input_ids: token ids tensor (flattened to 1D)\n",
    "        # - attention_mask: tensor indicating real tokens vs padding (flattened)\n",
    "        # - labels: tensor of the label (long integer type)\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KL10SQ9BefoH"
   },
   "source": [
    "# Purpose of the Code `BERTTextClassifier`\n",
    "\n",
    "The main purpose of this `BERTTextClassifier` code is to create a reusable **BERT-based text classification system** that can:\n",
    "\n",
    "- Load and preprocess text data (e.g., IMDb movie reviews).\n",
    "- Fine-tune a pretrained BERT model to classify texts into categories such as positive or negative sentiment.\n",
    "- Evaluate the trained model's performance on test data using metrics like accuracy and F1 score.\n",
    "- Make predictions with confidence scores (probabilities) on new, unseen text inputs.\n",
    "\n",
    "### Summary:\n",
    "This code implements a full pipeline for **fine-tuning and using BERT** for text classification tasks, wrapping data loading, training, evaluation, and prediction inside a single convenient class.\n",
    "\n",
    "---\n",
    "\n",
    "## Workflow:\n",
    "\n",
    "1. **Initialize the classifier**  \n",
    "   Load the BERT model, tokenizer, and move the model to the appropriate device (CPU/GPU).\n",
    "\n",
    "2. **Load IMDb data**  \n",
    "   Sample training and testing texts along with their labels.\n",
    "\n",
    "3. **Train the model**  \n",
    "   - Convert texts and labels into a dataset format.  \n",
    "   - Use `DataLoader` for batching data.  \n",
    "   - For each epoch and batch: perform a forward pass, compute loss, backpropagate, update weights, and adjust the learning rate.\n",
    "\n",
    "4. **Evaluate the model**  \n",
    "   - Run the model on test data without calculating gradients.  \n",
    "   - Collect predictions and compute metrics such as accuracy, F1 score, and classification report.\n",
    "\n",
    "5. **Predict new texts**  \n",
    "   - Tokenize new input texts, run them through the model, apply softmax to obtain probabilities, and return predicted classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "StaWtdUXTHPj"
   },
   "outputs": [],
   "source": [
    "# BERT Text Classifier\n",
    "class BERTTextClassifier:\n",
    "    \"\"\"BERT for Text Classification (Sentiment, Spam etc.)\"\"\"\n",
    "\n",
    "    # --- Initialize the classifier ---\n",
    "    def __init__(self, model_name='bert-base-uncased', num_classes=2, max_length=512):\n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Load the tokenizer for the specified pretrained BERT model\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "        # Load the pretrained BERT model for sequence classification with given number of classes\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\n",
    "            model_name, num_labels=num_classes\n",
    "        )\n",
    "        # Move the model to the available device (CPU or GPU)\n",
    "        self.model.to(device)\n",
    "\n",
    "    # --- Load IMDb data ---\n",
    "    def load_imdb_data(self, sample_size=5000):\n",
    "        \"\"\"Load IMDb movie reviews dataset\"\"\"\n",
    "\n",
    "        print(\"Loading IMDb dataset...\")\n",
    "\n",
    "        # Download the IMDb dataset using Hugging Face datasets library\n",
    "        dataset = load_dataset(\"imdb\")\n",
    "\n",
    "        # Randomly sample indices from the training set for quicker experiments\n",
    "        train_indices = np.random.choice(len(dataset['train']),\n",
    "                                       min(sample_size, len(dataset['train'])),\n",
    "                                       replace=False)\n",
    "\n",
    "        # Randomly sample indices from the test set (smaller subset)\n",
    "        test_indices = np.random.choice(len(dataset['test']),\n",
    "                                      min(sample_size//4, len(dataset['test'])),\n",
    "                                      replace=False)\n",
    "\n",
    "        # Extract texts and labels for training samples\n",
    "        train_texts = [dataset['train'][int(i)]['text'] for i in train_indices]\n",
    "        train_labels = [dataset['train'][int(i)]['label'] for i in train_indices]\n",
    "\n",
    "        # Extract texts and labels for test samples\n",
    "        test_texts = [dataset['test'][int(i)]['text'] for i in test_indices]\n",
    "        test_labels = [dataset['test'][int(i)]['label'] for i in test_indices]\n",
    "\n",
    "        print(f\"Train samples: {len(train_texts)}\")\n",
    "        print(f\"Test samples: {len(test_texts)}\")\n",
    "\n",
    "        return train_texts, train_labels, test_texts, test_labels\n",
    "\n",
    "    # --- Train the model ---\n",
    "    def train(self, train_texts, train_labels, epochs=1, batch_size=8, learning_rate=2e-5):\n",
    "        \"\"\"Train the text classifier\"\"\"\n",
    "\n",
    "        # Create dataset from raw texts and labels with tokenizer\n",
    "        train_dataset = TextClassificationDataset(\n",
    "            train_texts, train_labels, self.tokenizer, self.max_length\n",
    "        )\n",
    "\n",
    "        # DataLoader for batching and shuffling training data\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Optimizer with weight decay to update model parameters\n",
    "        optimizer = AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "        # Total number of training steps (batches * epochs)\n",
    "        total_steps = len(train_loader) * epochs\n",
    "\n",
    "        # Scheduler for gradually decreasing learning rate with warmup period\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=total_steps//10, num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "        self.model.train()  # Set model to training mode\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "\n",
    "            for batch in progress_bar:\n",
    "                optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "                # Move inputs and labels to device\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                # Forward pass: get model outputs and calculate loss\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "\n",
    "                loss = outputs.loss  # Extract loss value\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                loss.backward()  # Backpropagation\n",
    "\n",
    "                # Gradient clipping to avoid exploding gradients\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "\n",
    "                optimizer.step()  # Update model weights\n",
    "                scheduler.step()  # Update learning rate\n",
    "\n",
    "                # Show the current loss on the progress bar\n",
    "                progress_bar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "\n",
    "            print(f'Epoch {epoch+1}, Average Loss: {total_loss/len(train_loader):.4f}')\n",
    "\n",
    "    # --- Evaluate the model ---\n",
    "    def evaluate(self, test_texts, test_labels, batch_size=8):\n",
    "        \"\"\"Evaluate the text classifier\"\"\"\n",
    "\n",
    "        # Prepare dataset and dataloader for test set\n",
    "        test_dataset = TextClassificationDataset(\n",
    "            test_texts, test_labels, self.tokenizer, self.max_length\n",
    "        )\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        self.model.eval()  # Set model to evaluation mode\n",
    "\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            for batch in tqdm(test_loader, desc='Evaluating'):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                # Forward pass without labels (no loss computed)\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "\n",
    "                # Predicted class is the argmax of logits\n",
    "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "                predictions.extend(preds)  # Collect predictions\n",
    "                true_labels.extend(labels.cpu().numpy())  # Collect true labels\n",
    "\n",
    "        # Compute accuracy and F1 score for evaluation\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "        f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "        # Detailed classification report (precision, recall, F1 per class)\n",
    "        report = classification_report(true_labels, predictions,\n",
    "                                     target_names=['Negative', 'Positive'])\n",
    "\n",
    "        return accuracy, f1, report\n",
    "\n",
    "    # --- Predict new texts ---\n",
    "    def predict(self, texts):\n",
    "        \"\"\"Predict sentiment for new texts\"\"\"\n",
    "        predictions = []\n",
    "        probabilities = []\n",
    "\n",
    "        self.model.eval()  # Set model to eval mode\n",
    "\n",
    "        for text in texts:\n",
    "            # Tokenize input text, prepare tensors\n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Forward pass to get logits\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "\n",
    "                # Convert logits to probabilities using softmax\n",
    "                probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "                # Predicted class is argmax of logits\n",
    "                pred = torch.argmax(logits, dim=1).cpu().numpy()[0]\n",
    "\n",
    "                predictions.append(pred)       # Append predicted class\n",
    "                probabilities.append(probs)    # Append class probabilities\n",
    "\n",
    "        return predictions, probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZF9kCF-ETQ0"
   },
   "source": [
    "# Purpose of the `NERDataset` Class\n",
    "\n",
    "The `NERDataset` class is designed to prepare data for Named Entity Recognition (NER) tasks using transformer models like BERT.\n",
    "\n",
    "## Key goals:\n",
    "\n",
    "- **Input Handling:**  \n",
    "  Accepts pre-tokenized input sentences (`tokens_list`) and their corresponding word-level labels (`labels_list`).\n",
    "\n",
    "- **Tokenization & Alignment:**  \n",
    "  Uses a BERT-compatible tokenizer to split words into subword tokens while maintaining alignment between original word labels and tokenized subwords.\n",
    "\n",
    "- **Label Alignment:**  \n",
    "  Assigns the correct label to the first subword token of each word and marks subsequent subword tokens with `-100` so that they are ignored during loss calculation.\n",
    "\n",
    "- **Padding & Truncation:**  \n",
    "  Ensures that all sequences and their aligned labels have a consistent length (`max_length`) by padding or truncating as needed.\n",
    "\n",
    "- **Output Format:**  \n",
    "  Provides data samples as dictionaries containing `input_ids`, `attention_mask`, and `labels` tensors, ready to be fed directly into transformer models for training or evaluation.\n",
    "\n",
    "## Why is this important?\n",
    "\n",
    "Transformer tokenizers often split words into multiple subwords. For sequence labeling tasks like NER, the model expects token-level labels aligned with these subwords. This class automates the process of properly aligning labels to tokenized inputs, which is crucial for effective training of token classification models.\n",
    "\n",
    "\n",
    "# Token Classification Label Alignment with BERT\n",
    "\n",
    "This example demonstrates how to handle token classification tasks (like Named Entity Recognition - NER) using BERT's tokenizer, including how to align original word-level labels with BERT's subword tokens.\n",
    "\n",
    "---\n",
    "\n",
    "## Input Example\n",
    "\n",
    "**Sentence (tokens):**  \n",
    "`[\"John\", \"lives\", \"in\", \"London\"]`\n",
    "\n",
    "**Labels (per token):**  \n",
    "`[1, 0, 0, 2]`\n",
    "\n",
    "- `1` = B-PER (Beginning of a Person entity, e.g., \"John\")  \n",
    "- `0` = O (Outside any entity)  \n",
    "- `2` = B-LOC (Beginning of a Location entity, e.g., \"London\")\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Tokenizer Output\n",
    "\n",
    "| Original Words | John | lives | in | London |\n",
    "|----------------|-------|-------|----|--------|\n",
    "| BERT Tokens    | `[CLS]`, John, lives, in, Lon, `##don`, `[SEP]`, `[PAD]`... |\n",
    "| Word IDs       | None  | 0     | 1  | 2      | 3    | 3       | None    | None    |\n",
    "\n",
    "- BERT splits \"London\" into two subword tokens: `Lon` and `##don`.\n",
    "- Special tokens like `[CLS]` (start) and `[SEP]` (end) have no word IDs.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Label Alignment\n",
    "\n",
    "| Token      | Word ID | Original Label | Final Label for Model |\n",
    "|------------|---------|----------------|-----------------------|\n",
    "| `[CLS]`    | None    | —              | -100 (ignore)         |\n",
    "| John       | 0       | 1 (B-PER)      | 1                     |\n",
    "| lives      | 1       | 0 (O)          | 0                     |\n",
    "| in         | 2       | 0 (O)          | 0                     |\n",
    "| Lon        | 3       | 2 (B-LOC)      | 2                     |\n",
    "| ##don      | 3       | 2 (B-LOC)      | -100 (ignore)         |\n",
    "| `[SEP]`    | None    | —              | -100 (ignore)         |\n",
    "| `[PAD]`    | None    | —              | -100 (ignore)         |\n",
    "\n",
    "- The first subword token of a word gets the original label.\n",
    "- Continuation subwords (like `##don`) get `-100` to be ignored during loss calculation.\n",
    "- Special tokens and padding tokens also get `-100`.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Model Input Example\n",
    "\n",
    "```python\n",
    "{\n",
    "  'input_ids': tensor([...]),           # Token IDs for BERT tokens (PyTorch tensor)\n",
    "  'attention_mask': tensor([1, 1, 1, ...]),  # 1 for real tokens, 0 for padding\n",
    "  'labels': tensor([-100, 1, 0, 0, 2, -100, -100, ...])  # Labels aligned with tokens\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfvPhWd5Tz2s"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    \"\"\"Dataset class for Named Entity Recognition (NER) tasks\"\"\"\n",
    "\n",
    "    def __init__(self, tokens_list, labels_list, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with:\n",
    "        - tokens_list: list of tokenized sentences (list of tokens per sentence)\n",
    "        - labels_list: corresponding list of label sequences for each sentence\n",
    "        - tokenizer: tokenizer compatible with BERT (or similar) model\n",
    "        - max_length: max token sequence length (for padding/truncation)\n",
    "        \"\"\"\n",
    "        self.tokens_list = tokens_list\n",
    "        self.labels_list = labels_list\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return number of samples in the dataset\n",
    "        return len(self.tokens_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve tokens and labels for the given index\n",
    "        tokens = self.tokens_list[idx]\n",
    "        labels = self.labels_list[idx]\n",
    "\n",
    "        # Tokenize input tokens with padding and truncation\n",
    "        # `is_split_into_words=True` tells tokenizer that input is already tokenized at word-level\n",
    "        encoding = self.tokenizer(\n",
    "            tokens,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            is_split_into_words=True,\n",
    "            return_tensors='pt'  # Return PyTorch tensors\n",
    "        )\n",
    "\n",
    "        # word_ids maps each tokenized subword token back to the original word index (or None for special tokens)\n",
    "        word_ids = encoding.word_ids(batch_index=0)\n",
    "\n",
    "        aligned_labels = []  # List to store labels aligned with subword tokens\n",
    "        previous_word_idx = None\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # Special tokens like [CLS], [SEP] get label -100 (ignored in loss)\n",
    "                aligned_labels.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # For the first subword token of a word, assign the original word's label\n",
    "                # Defensive check in case word_idx exceeds labels length (assign 0 if so)\n",
    "                aligned_labels.append(labels[word_idx] if word_idx < len(labels) else 0)\n",
    "            else:\n",
    "                # For subsequent subword tokens of the same word, assign -100 to ignore them\n",
    "                aligned_labels.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        # Ensure aligned labels list matches max_length by padding or truncating\n",
    "        if len(aligned_labels) < self.max_length:\n",
    "            aligned_labels += [-100] * (self.max_length - len(aligned_labels))\n",
    "        elif len(aligned_labels) > self.max_length:\n",
    "            aligned_labels = aligned_labels[:self.max_length]\n",
    "\n",
    "        # Return dictionary with input IDs, attention mask, and aligned labels as tensors\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),        # Token IDs tensor shape: (max_length,)\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),  # Attention mask tensor shape: (max_length,)\n",
    "            'labels': torch.tensor(aligned_labels, dtype=torch.long)  # Aligned labels tensor shape: (max_length,)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NNL8w1sG8hX"
   },
   "source": [
    "| **Label**  | **Full Form**          | **Meaning (English)**                                                   | **Example**                                                       |\n",
    "|------------|------------------------|------------------------------------------------------------------------|------------------------------------------------------------------|\n",
    "| **O**      | Outside                | Not part of any named entity (normal word)                             | \"works\", \"at\"                                                    |\n",
    "| **B-PER**  | Begin - Person         | First word of a person’s name                                          | \"John\" → `B-PER`                                                 |\n",
    "| **I-PER**  | Inside - Person        | Continuation word(s) of a person’s name                                | \"Mary Jane\" → `Mary = B-PER`, `Jane = I-PER`                     |\n",
    "| **B-ORG**  | Begin - Organization   | First word of an organization or company name                          | \"Google\" → `B-ORG`                                               |\n",
    "| **I-ORG**  | Inside - Organization  | Continuation word(s) of an organization’s name                         | \"New York Times\" → `New = B-ORG`, `York = I-ORG`, `Times = I-ORG`|\n",
    "| **B-LOC**  | Begin - Location       | First word of a location or place name                                 | \"London\" → `B-LOC`                                               |\n",
    "| **I-LOC**  | Inside - Location      | Continuation word(s) of a location name                                | \"New York\" → `New = B-LOC`, `York = I-LOC`                       |\n",
    "| **B-MISC** | Begin - Miscellaneous  | First word of miscellaneous entities (events, products, nationalities) | \"Indian\" (nationality) → `B-MISC`                                |\n",
    "| **I-MISC** | Inside - Miscellaneous | Continuation word(s) of miscellaneous entities                         | \"South Korean\" → `South = B-MISC`, `Korean = I-MISC`             |\n",
    "\n",
    "\n",
    "\n",
    "# Explanation of NER Labels\n",
    "\n",
    "- **B** → Begin: The first word of a named entity (the starting word of the entity).\n",
    "- **I** → Inside: Continuation words that belong to the same named entity.\n",
    "- **O** → Outside: Words that are not part of any named entity (normal words).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📚 About WikiAnn Dataset\n",
    "\n",
    "The **WikiAnn** dataset is a multilingual Named Entity Recognition (NER) dataset created by combining Wikipedia articles with manual and automatic annotations. It is widely used for training and evaluating NER models across multiple languages.\n",
    "\n",
    "---\n",
    "\n",
    "# 📌 Purpose of the `BERTNERClassifier` Code\n",
    "\n",
    "This code implements a **Named Entity Recognition (NER)** pipeline using a fine-tuned **BERT** model on the **WikiAnn (English)** dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Goal\n",
    "\n",
    "To automatically **identify and label named entities** (like person names, locations, and organizations) in raw text using a BERT-based deep learning model.\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ What the Code Does\n",
    "\n",
    "### 1. Load the Dataset\n",
    "- Loads the [WikiAnn English](https://huggingface.co/datasets/wikiann) dataset, which contains:\n",
    "  - Sentences broken into **tokens**\n",
    "  - Corresponding **NER labels** such as:\n",
    "    - `B-PER` (beginning of a person’s name)\n",
    "    - `I-LOC` (continuation of a location)\n",
    "    - `O` (non-entity words)\n",
    "\n",
    "### 2. Define a Classifier\n",
    "- The `BERTNERClassifier` class is responsible for:\n",
    "  - Loading a pre-trained **BERT** model (`bert-base-uncased`)\n",
    "  - Modifying it for **token classification**\n",
    "  - Tokenizing inputs\n",
    "  - Managing label definitions\n",
    "\n",
    "### 3. Train the Model\n",
    "- Fine-tunes the BERT model using:\n",
    "  - A **sampled subset** of WikiAnn\n",
    "  - `AdamW` optimizer with weight decay\n",
    "  - **Learning rate scheduler** for gradual warmup\n",
    "  - **Gradient clipping** to prevent exploding gradients\n",
    "\n",
    "### 4. Evaluate the Model\n",
    "- Tests the model’s performance on unseen data\n",
    "- Calculates:\n",
    "  - **Accuracy**\n",
    "  - **F1-score (weighted)**\n",
    "- Ignores padded tokens for a fair evaluation\n",
    "\n",
    "### 5. Predict on New Text\n",
    "- Accepts tokenized sentences (word lists)\n",
    "- Returns predicted NER tags aligned with original tokens\n",
    "- Handles alignment between subword tokens and full words\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Real-World Application\n",
    "\n",
    "This pipeline can be used in various real-world scenarios like:\n",
    "\n",
    "- 🗂️ **Information extraction** from unstructured documents  \n",
    "- 🤖 **Conversational agents** that identify people, places, or companies  \n",
    "- 📑 **Document classifiers** with entity context awareness  \n",
    "- 🔍 **Search engines** with better indexing using named entities  \n",
    "- 📢 **Social media monitoring** tools to detect key mentions in text\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 Summary Table\n",
    "\n",
    "| Component         | Description |\n",
    "|------------------|-------------|\n",
    "| **Dataset**       | WikiAnn (English) |\n",
    "| **Model**         | BERT (`bert-base-uncased`) |\n",
    "| **NER Tags**      | `B-PER`, `I-LOC`, `B-ORG`, `O`, etc. |\n",
    "| **Core Functions**| Load data, train, evaluate, and predict |\n",
    "| **Libraries Used**| `transformers`, `datasets`, `sklearn`, `torch`, `tqdm` |\n",
    "\n",
    "---\n",
    "\n",
    "> 💡 **Tip:** You can easily extend this pipeline to other languages or entity types by switching the WikiAnn language version or adjusting the label list.\n",
    "\n",
    "---\n",
    "\n",
    "# 🔁 Complete Flow of the Classifier\n",
    "\n",
    "1. Load the **WikiAnn English dataset** from Hugging Face using `load_dataset`.\n",
    "2. Randomly sample:\n",
    "   - **1000 training examples**\n",
    "   - **250 test examples**\n",
    "3. Extract:\n",
    "   - **Tokens**: Words in each sentence  \n",
    "   - **NER labels**: Tags for each word\n",
    "4. Return token-label pairs to use in:\n",
    "   - Training (`.train()`)\n",
    "   - Evaluation (`.evaluate()`)\n",
    "   - Inference (`.predict()`)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84WYdrwaT1Mv"
   },
   "outputs": [],
   "source": [
    "# Define the main class for BERT-based Named Entity Recognition\n",
    "class BERTNERClassifier:\n",
    "    \"\"\"\n",
    "    A classifier for Named Entity Recognition using BERT.\n",
    "\n",
    "    It includes methods for:\n",
    "    - Loading and sampling the WikiAnn dataset\n",
    "    - Fine-tuning the model\n",
    "    - Evaluating it on test data\n",
    "    - Making predictions on new text\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------------- Initialization ----------------\n",
    "    def __init__(self, model_name='bert-base-uncased', num_labels=9, max_length=512):\n",
    "        # model_name: the pre-trained BERT model to use\n",
    "        # num_labels: number of unique entity tags (e.g. B-PER, I-LOC, etc.)\n",
    "        # max_length: max sequence length for padding/truncation\n",
    "        self.model_name = model_name\n",
    "        self.num_labels = num_labels\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Load tokenizer for BERT\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "        # Load BERT model for token classification\n",
    "        self.model = BertForTokenClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "        self.model.to(device)\n",
    "\n",
    "        # Define the list of NER labels as per WikiAnn format\n",
    "        self.labels = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG',\n",
    "                       'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
    "        \n",
    "    # ---------------- Dataset Loader ----------------\n",
    "    def load_wikiann_data(self, sample_size=1000):\n",
    "        # sample_size: number of examples to sample from the dataset\n",
    "        print(\"Loading WikiAnn English dataset...\")\n",
    "        dataset = load_dataset(\"wikiann\", \"en\")  # Load WikiAnn NER dataset for English\n",
    "\n",
    "        # Randomly choose training and test indices\n",
    "        train_indices = np.random.choice(len(dataset['train']),\n",
    "                                         min(sample_size, len(dataset['train'])),\n",
    "                                         replace=False)\n",
    "        test_indices = np.random.choice(len(dataset['test']),\n",
    "                                        min(sample_size // 4, len(dataset['test'])),\n",
    "                                        replace=False)\n",
    "\n",
    "        # Extract tokens and their corresponding NER labels\n",
    "        train_tokens = [dataset['train'][int(i)]['tokens'] for i in train_indices]\n",
    "        train_labels = [dataset['train'][int(i)]['ner_tags'] for i in train_indices]\n",
    "        test_tokens = [dataset['test'][int(i)]['tokens'] for i in test_indices]\n",
    "        test_labels = [dataset['test'][int(i)]['ner_tags'] for i in test_indices]\n",
    "\n",
    "        print(f\"Train samples: {len(train_tokens)}\")\n",
    "        print(f\"Test samples: {len(test_tokens)}\")\n",
    "\n",
    "        return train_tokens, train_labels, test_tokens, test_labels\n",
    "    \n",
    "    # ---------------- Training Function ----------------\n",
    "    def train(self, train_tokens, train_labels, epochs=1, batch_size=8, learning_rate=2e-5):\n",
    "        # Fine-tunes the BERT model using the provided training data\n",
    "        # train_tokens: list of word-token lists\n",
    "        # train_labels: list of NER tag sequences\n",
    "        # epochs: number of training passes\n",
    "        # batch_size: number of samples per batch\n",
    "        # learning_rate: optimizer learning rate\n",
    "\n",
    "        train_dataset = NERDataset(train_tokens, train_labels, self.tokenizer, self.max_length)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Setup Adam optimizer and learning rate scheduler\n",
    "        optimizer = AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "        total_steps = len(train_loader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=total_steps // 10, num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "        self.model.train()  # Set model to training mode\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "            for batch in progress_bar:\n",
    "                optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss  # Compute loss\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                loss.backward()  # Backpropagation\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)  # Gradient clipping\n",
    "                optimizer.step()  # Update weights\n",
    "                scheduler.step()  # Update learning rate\n",
    "\n",
    "                progress_bar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}, Avg Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # ---------------- Evaluation Function ----------------       \n",
    "    def evaluate(self, test_tokens, test_labels, batch_size=8):\n",
    "        # Evaluates the model's performance on unseen test data\n",
    "        # Returns overall accuracy and weighted F1 score\n",
    "\n",
    "        test_dataset = NERDataset(test_tokens, test_labels, self.tokenizer, self.max_length)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        self.model.eval()  # Set model to evaluation mode\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits  # Raw scores\n",
    "                preds = torch.argmax(logits, dim=2).cpu().numpy()  # Predicted class indices\n",
    "                labels = labels.cpu().numpy()\n",
    "\n",
    "                # Filter out padding tokens (label = -100)\n",
    "                for i in range(preds.shape[0]):\n",
    "                    for j in range(preds.shape[1]):\n",
    "                        if labels[i][j] != -100:\n",
    "                            predictions.append(preds[i][j])\n",
    "                            true_labels.append(labels[i][j])\n",
    "\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "        f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "        return accuracy, f1\n",
    "    \n",
    "    # ---------------- Prediction Function ----------------\n",
    "    def predict(self, tokens_list):\n",
    "        # Predict NER tags for a list of tokenized word lists (new sentences)\n",
    "\n",
    "        predictions = []\n",
    "        self.model.eval()\n",
    "\n",
    "        for tokens in tokens_list:\n",
    "            encoding = self.tokenizer(\n",
    "                tokens,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt',\n",
    "                is_split_into_words=True\n",
    "            )\n",
    "\n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                preds = torch.argmax(logits, dim=2).cpu().numpy()[0]\n",
    "\n",
    "                # Get predicted tags aligned with original words\n",
    "                word_ids = encoding.word_ids(batch_index=0)\n",
    "                token_predictions = []\n",
    "                previous_word_idx = None\n",
    "\n",
    "                for i, word_idx in enumerate(word_ids):\n",
    "                    if word_idx is not None and word_idx != previous_word_idx:\n",
    "                        if word_idx < len(tokens):  # Avoid indexing errors\n",
    "                            token_predictions.append(self.labels[preds[i]])\n",
    "                    previous_word_idx = word_idx\n",
    "\n",
    "                predictions.append(token_predictions)\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of `QADataset` Class\n",
    "\n",
    "## 🎯 Purpose\n",
    "\n",
    "The `QADataset` class prepares data for a **Question Answering (QA)** task (like SQuAD). It helps a model learn **where the answer is located within a given context** by:\n",
    "\n",
    "- Tokenizing the question and context together.\n",
    "- Finding the start and end token positions of the answer in the context.\n",
    "- Returning this data so the model can learn to predict the answer span.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 What the Code Does\n",
    "\n",
    "1. **Inputs:**  \n",
    "   - A list of questions  \n",
    "   - Corresponding contexts (paragraphs containing answers)  \n",
    "   - Answers with their text and character start positions  \n",
    "   - A tokenizer for converting text into tokens  \n",
    "   - Max sequence length (for padding/truncation)  \n",
    "\n",
    "2. **Tokenization:**  \n",
    "   - Combines question and context  \n",
    "   - Tokenizes into input IDs and attention masks  \n",
    "   - Provides offset mapping to link tokens back to original characters  \n",
    "\n",
    "3. **Answer Span Identification:**  \n",
    "   - Uses offset mapping to find tokens that cover the answer text  \n",
    "   - Determines start and end token indices for the answer  \n",
    "\n",
    "4. **Returns:**  \n",
    "   A dictionary containing:  \n",
    "   - `input_ids`: tokenized question + context  \n",
    "   - `attention_mask`: mask for padding tokens  \n",
    "   - `start_positions`: index of token where answer starts  \n",
    "   - `end_positions`: index of token where answer ends  \n",
    "\n",
    "---\n",
    "\n",
    "## 📝 Simple Example\n",
    "\n",
    "| Input     | Example Value                                |\n",
    "|-----------|----------------------------------------------|\n",
    "| Question  | \"Where is the Eiffel Tower located?\"         |\n",
    "| Context   | \"The Eiffel Tower is located in Paris, France.\" |\n",
    "| Answer    | \"Paris\" (starts at character 27 in context)  |\n",
    "\n",
    "### How it works:\n",
    "\n",
    "- The tokenizer converts question + context into tokens like:  \n",
    "  `[CLS] Where is the Eiffel Tower located? The Eiffel Tower is located in Paris, France. [SEP]`\n",
    "\n",
    "- The code finds the tokens that cover `\"Paris\"` by matching character offsets.\n",
    "\n",
    "- Sets:  \n",
    "  - `start_positions = 12` (token where \"Paris\" starts)  \n",
    "  - `end_positions = 12` (same for a single-word answer)\n",
    "\n",
    "- These positions help the model learn to point to the answer span during training.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔑 Why Is This Important?\n",
    "\n",
    "- QA models need to **locate exact answer spans** inside paragraphs.\n",
    "- This dataset class formats data correctly to train such models by providing tokenized inputs and precise answer token positions.\n",
    "\n",
    "---\n",
    "\n",
    "> **Analogy:**  \n",
    "> If someone asks you, \"Which word in the sentence is the answer?\" you’d count and say \"word 12.\" This class teaches the model to do the same with tokens.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can show you a code snippet to visualize tokenization and answer span finding!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "noGChIBBUvKx"
   },
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    \"\"\"Dataset class for Question Answering tasks (SQuAD-style)\"\"\"\n",
    "\n",
    "    def __init__(self, questions, contexts, answers, tokenizer, max_length=512):\n",
    "        # Initialize the dataset with:\n",
    "        # questions: list of question strings\n",
    "        # contexts: list of context paragraphs (each containing the answer)\n",
    "        # answers: list of dicts with 'text' and 'answer_start' (character-level answer positions)\n",
    "        # tokenizer: tokenizer to convert text into tokens suitable for model input\n",
    "        # max_length: max number of tokens in input sequence (for padding/truncation)\n",
    "        self.questions = questions\n",
    "        self.contexts = contexts\n",
    "        self.answers = answers\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns the total number of samples (questions) in the dataset\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Fetch the idx-th sample: question, context, and answer info\n",
    "        question = self.questions[idx]\n",
    "        context = self.contexts[idx]\n",
    "        answer = self.answers[idx]  # dict like {'text': [...], 'answer_start': [...]}\n",
    "\n",
    "        # Tokenize the question and context together, producing:\n",
    "        # - input_ids: token IDs of question + context\n",
    "        # - attention_mask: mask for real tokens vs padding\n",
    "        # - offset_mapping: maps tokens back to character positions in original text\n",
    "        encoding = self.tokenizer(\n",
    "            question,\n",
    "            context,\n",
    "            truncation=True,              # truncate to max_length if too long\n",
    "            padding='max_length',         # pad shorter sequences to max_length\n",
    "            max_length=self.max_length,   # maximum allowed length for input\n",
    "            return_offsets_mapping=True,  # get token-to-char position mapping\n",
    "            return_tensors='pt'           # return PyTorch tensors\n",
    "        )\n",
    "\n",
    "        # offset_mapping has shape (1, max_length, 2), pop to remove from encoding dict\n",
    "        # [0] extracts the sequence dimension since batch size = 1 here\n",
    "        offset_mapping = encoding.pop(\"offset_mapping\")[0]  # shape: (max_length, 2)\n",
    "\n",
    "        # Default start and end token positions (for answers not found)\n",
    "        start_positions = torch.tensor(0, dtype=torch.long)\n",
    "        end_positions = torch.tensor(0, dtype=torch.long)\n",
    "\n",
    "        # If an answer exists and answer start positions are provided...\n",
    "        if answer and 'answer_start' in answer and answer['answer_start']:\n",
    "            # Extract character index where answer starts in context\n",
    "            answer_start = answer['answer_start'][0]\n",
    "            # Extract the answer text string\n",
    "            answer_text = answer['text'][0]\n",
    "            # Calculate the character index where answer ends\n",
    "            answer_end = answer_start + len(answer_text)\n",
    "\n",
    "            # Iterate through offset mappings to find the token span covering the answer\n",
    "            for idx, (start, end) in enumerate(offset_mapping):\n",
    "                # Find token whose char span includes the answer start char\n",
    "                if start <= answer_start < end:\n",
    "                    start_positions = torch.tensor(idx, dtype=torch.long)\n",
    "                # Find token whose char span includes the answer end char\n",
    "                if start < answer_end <= end:\n",
    "                    end_positions = torch.tensor(idx, dtype=torch.long)\n",
    "                    break  # Once end token found, stop searching\n",
    "\n",
    "        # Return a dict with inputs for model training/inference:\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),      # token ids, shape: (max_length,)\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),  # attention mask, shape: (max_length,)\n",
    "            'start_positions': start_positions,                  # index of answer start token\n",
    "            'end_positions': end_positions                       # index of answer end token\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose of the BERT Question Answering Code\n",
    "\n",
    "---\n",
    "\n",
    "### What is this code for?\n",
    "\n",
    "This code builds a **Question Answering (QA) system** using a BERT model.  \n",
    "You provide:\n",
    "\n",
    "- A **context** (paragraph or text passage)  \n",
    "- A **question** related to that context  \n",
    "\n",
    "The system finds the **exact answer span** within the context.\n",
    "\n",
    "---\n",
    "\n",
    "### Why is this useful?\n",
    "\n",
    "You can use this for:\n",
    "\n",
    "- Smart assistants that answer questions based on documents  \n",
    "- Search engines that find specific answers, not just documents  \n",
    "- Chatbots that understand and answer factual questions  \n",
    "\n",
    "---\n",
    "\n",
    "### How does it work? (Simple steps)\n",
    "\n",
    "1. **Load data:** Uses the SQuAD dataset containing questions, contexts, and exact answers.  \n",
    "2. **Train:** Fine-tunes BERT to learn where answers start and end inside paragraphs.  \n",
    "3. **Answer:** Given a new question and paragraph, predicts the text span that answers the question.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Idea\n",
    "\n",
    "The model doesn’t generate text from scratch — it **selects a snippet** (span) from the given paragraph as the answer, similar to highlighting a phrase in a book.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xXlml3TfVDVH"
   },
   "outputs": [],
   "source": [
    "class BERTQuestionAnswering:\n",
    "    \"\"\"BERT model wrapper for Question Answering tasks (SQuAD style)\"\"\"\n",
    "\n",
    "    # ---------------- Initialization ----------------\n",
    "    def __init__(self, model_name='bert-base-uncased', max_length=512):\n",
    "        \"\"\"\n",
    "        Initialize the tokenizer and model.\n",
    "\n",
    "        Parameters:\n",
    "        - model_name (str): Pretrained BERT model name from Hugging Face (default 'bert-base-uncased').\n",
    "        - max_length (int): Maximum token length for input sequences (default 512).\n",
    "        \"\"\"\n",
    "        self.model_name = model_name  # Store model name\n",
    "        self.max_length = max_length  # Store max token length for inputs\n",
    "\n",
    "        # Load pretrained tokenizer to convert text to tokens/ids\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "        # Load pretrained BERT model with a QA head (predicts start/end tokens)\n",
    "        self.model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "        # Send model to GPU if available, else CPU\n",
    "        self.model.to(device)\n",
    "\n",
    "    # ---------------- Dataset Loading ----------------\n",
    "    def load_squad_data(self, sample_size=2000):\n",
    "        \"\"\"\n",
    "        Load and sample the SQuAD dataset.\n",
    "\n",
    "        Steps:\n",
    "        - Loads the full SQuAD dataset.\n",
    "        - Randomly selects a subset of training and validation samples for faster experiments.\n",
    "        - Extracts questions, contexts, and answers from the dataset.\n",
    "\n",
    "        Parameters:\n",
    "        - sample_size (int): Number of training samples to randomly select (default 2000).\n",
    "\n",
    "        Returns:\n",
    "        - train_questions, train_contexts, train_answers: training data lists.\n",
    "        - val_questions, val_contexts, val_answers: validation data lists.\n",
    "        \"\"\"\n",
    "        print(\"Loading SQuAD dataset...\")\n",
    "\n",
    "        # Load the full SQuAD dataset from Hugging Face datasets\n",
    "        dataset = load_dataset(\"squad\")\n",
    "\n",
    "        # Randomly select sample_size examples from training set without replacement\n",
    "        train_indices = np.random.choice(len(dataset['train']),\n",
    "                                         min(sample_size, len(dataset['train'])),\n",
    "                                         replace=False)\n",
    "        # Randomly select 1/4th sample size for validation set\n",
    "        val_indices = np.random.choice(len(dataset['validation']),\n",
    "                                       min(sample_size // 4, len(dataset['validation'])),\n",
    "                                       replace=False)\n",
    "\n",
    "        # Extract question texts for selected training samples\n",
    "        train_questions = [dataset['train'][int(i)]['question'] for i in train_indices]\n",
    "        # Extract context paragraphs for selected training samples\n",
    "        train_contexts = [dataset['train'][int(i)]['context'] for i in train_indices]\n",
    "        # Extract answers for selected training samples (dict with text and start positions)\n",
    "        train_answers = [dataset['train'][int(i)]['answers'] for i in train_indices]\n",
    "\n",
    "        # Do the same extraction for validation samples\n",
    "        val_questions = [dataset['validation'][int(i)]['question'] for i in val_indices]\n",
    "        val_contexts = [dataset['validation'][int(i)]['context'] for i in val_indices]\n",
    "        val_answers = [dataset['validation'][int(i)]['answers'] for i in val_indices]\n",
    "\n",
    "        # Print counts for user info\n",
    "        print(f\"Train samples: {len(train_questions)}\")\n",
    "        print(f\"Validation samples: {len(val_questions)}\")\n",
    "\n",
    "        # Return extracted lists\n",
    "        return (train_questions, train_contexts, train_answers,\n",
    "                val_questions, val_contexts, val_answers)\n",
    "    \n",
    "    # ---------------- Training ----------------\n",
    "    def train(self, questions, contexts, answers, epochs=1, batch_size=8, learning_rate=2e-5):\n",
    "        \"\"\"\n",
    "        Fine-tune the BERT QA model on the provided data.\n",
    "\n",
    "        Steps:\n",
    "        - Wrap the data into a custom Dataset (QADataset) for tokenization and label processing.\n",
    "        - Use DataLoader for batching and shuffling.\n",
    "        - Define optimizer (AdamW) and learning rate scheduler.\n",
    "        - Run training epochs with loss backpropagation and gradient clipping.\n",
    "        - Print progress and average loss per epoch.\n",
    "\n",
    "        Parameters:\n",
    "        - questions (list): List of question strings.\n",
    "        - contexts (list): List of context strings.\n",
    "        - answers (list): List of answer dicts containing 'text' and 'answer_start'.\n",
    "        - epochs (int): Number of training epochs.\n",
    "        - batch_size (int): Batch size for DataLoader.\n",
    "        - learning_rate (float): Optimizer learning rate.\n",
    "        \"\"\"\n",
    "        # Create the dataset with questions, contexts, answers, and tokenizer\n",
    "        train_dataset = QADataset(questions, contexts, answers, self.tokenizer, self.max_length)\n",
    "        # Create data loader to iterate over dataset in batches and shuffle for randomness\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Initialize AdamW optimizer with weight decay (better for transformers)\n",
    "        optimizer = AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "        # Total training steps = number of batches per epoch * epochs\n",
    "        total_steps = len(train_loader) * epochs\n",
    "\n",
    "        # Learning rate scheduler to warm-up and then decay linearly\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=total_steps // 10, num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "        self.model.train()  # Set model to training mode\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0  # Track cumulative loss per epoch\n",
    "            # Progress bar to monitor training progress\n",
    "            progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}')\n",
    "\n",
    "            for batch in progress_bar:\n",
    "                optimizer.zero_grad()  # Reset gradients before each batch\n",
    "\n",
    "                # Move batch data to device\n",
    "                input_ids = batch['input_ids'].to(device)           # Token ids for question+context\n",
    "                attention_mask = batch['attention_mask'].to(device) # Mask to ignore padding tokens\n",
    "                start_positions = batch['start_positions'].to(device) # True start token index of answer\n",
    "                end_positions = batch['end_positions'].to(device)     # True end token index of answer\n",
    "\n",
    "                # Forward pass through BERT model\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,             # Input tokens batch\n",
    "                    attention_mask=attention_mask,   # Attention mask batch\n",
    "                    start_positions=start_positions, # True start token indices (labels)\n",
    "                    end_positions=end_positions      # True end token indices (labels)\n",
    "                )\n",
    "\n",
    "                loss = outputs.loss  # Compute loss between predicted and true start/end positions\n",
    "\n",
    "                total_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "                loss.backward()  # Backpropagation to compute gradients\n",
    "\n",
    "                # Clip gradients to max norm 1.0 to stabilize training\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "\n",
    "                optimizer.step()  # Update model parameters based on gradients\n",
    "\n",
    "                scheduler.step()  # Update learning rate according to scheduler\n",
    "\n",
    "                # Update progress bar with current loss value\n",
    "                progress_bar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "\n",
    "            # Print average loss at end of epoch\n",
    "            print(f'Epoch {epoch + 1}, Average Loss: {total_loss / len(train_loader):.4f}')\n",
    "\n",
    "    # ---------------- Inference / Prediction ----------------\n",
    "    def answer_question(self, question, context, max_answer_len=30):\n",
    "        \"\"\"\n",
    "        Predict answer span for a single question and context pair.\n",
    "\n",
    "        Steps:\n",
    "        - Tokenize and encode input question and context.\n",
    "        - Pass through BERT QA model to get start and end logits.\n",
    "        - Select token positions with highest start and end scores.\n",
    "        - Ensure valid span and limit maximum answer length.\n",
    "        - Decode predicted tokens back into text answer.\n",
    "\n",
    "        Parameters:\n",
    "        - question (str): Question string.\n",
    "        - context (str): Context paragraph string.\n",
    "        - max_answer_len (int): Max allowed answer length in tokens.\n",
    "\n",
    "        Returns:\n",
    "        - answer (str): Predicted answer text extracted from context.\n",
    "        \"\"\"\n",
    "        # Tokenize and encode question + context as model input\n",
    "        encoding = self.tokenizer(\n",
    "            question,\n",
    "            context,\n",
    "            truncation=True,              # Truncate if longer than max_length\n",
    "            padding='max_length',         # Pad sequences to max_length\n",
    "            max_length=self.max_length,   # Max tokens length\n",
    "            return_tensors='pt'           # Return PyTorch tensors\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].to(device)             # Token IDs (1 x max_length)\n",
    "        attention_mask = encoding['attention_mask'].to(device)   # Attention mask for padding\n",
    "\n",
    "        self.model.eval()  # Set model to evaluation mode (disables dropout)\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation for inference\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,           # Input token IDs\n",
    "                attention_mask=attention_mask  # Attention mask\n",
    "            )\n",
    "\n",
    "            start_logits = outputs.start_logits  # Start position logits (1 x max_length)\n",
    "            end_logits = outputs.end_logits      # End position logits (1 x max_length)\n",
    "\n",
    "            # Pick token with highest start logit score as predicted start index\n",
    "            start_idx = torch.argmax(start_logits, dim=1).item()\n",
    "            # Pick token with highest end logit score as predicted end index\n",
    "            end_idx = torch.argmax(end_logits, dim=1).item()\n",
    "\n",
    "            # Correct invalid spans where end comes before start\n",
    "            if end_idx < start_idx:\n",
    "                end_idx = start_idx\n",
    "\n",
    "            # Limit answer length to max_answer_len tokens\n",
    "            if (end_idx - start_idx) > max_answer_len:\n",
    "                end_idx = start_idx + max_answer_len\n",
    "\n",
    "            # Extract predicted token ids for the answer span\n",
    "            answer_tokens = input_ids[0][start_idx:end_idx + 1]\n",
    "\n",
    "            # Decode tokens to human-readable string, skipping special tokens like [CLS], [SEP]\n",
    "            answer = self.tokenizer.decode(answer_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "            return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of `run_text_classification_demo()` Function\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This function runs a **demo of text classification**, specifically sentiment analysis, using a BERT-based classifier.  \n",
    "It walks through loading data, training a model, evaluating it, and testing it on custom text samples.\n",
    "\n",
    "---\n",
    "\n",
    "### Workflow Breakdown\n",
    "\n",
    "1. **Print Demo Header**  \n",
    "   Displays a clear, formatted title for the demo in the console.\n",
    "\n",
    "2. **Initialize Classifier**  \n",
    "   Creates an instance of the `BERTTextClassifier` with 2 classes (Positive, Negative).\n",
    "\n",
    "3. **Load Data**  \n",
    "   Calls `load_imdb_data()` on the classifier to get a subset of the IMDB movie reviews dataset, with:\n",
    "   - `train_texts`: training review texts  \n",
    "   - `train_labels`: their corresponding sentiment labels (0=Negative, 1=Positive)  \n",
    "   - `test_texts`: testing review texts  \n",
    "   - `test_labels`: testing sentiment labels  \n",
    "   *Note:* Only 1000 samples are loaded here for a quick demo.\n",
    "\n",
    "4. **Display Sample**  \n",
    "   Prints the first training review (first 200 characters) and its sentiment label.\n",
    "\n",
    "5. **Train the Model**  \n",
    "   Trains the classifier on the training data for 1 epoch with a batch size of 8.  \n",
    "   This fine-tunes BERT to classify sentiments in text.\n",
    "\n",
    "6. **Evaluate the Model**  \n",
    "   Evaluates the trained model on the test data to calculate:\n",
    "   - Accuracy (percentage of correct predictions)  \n",
    "   - F1 Score (balance between precision and recall)  \n",
    "   - A detailed classification report (precision, recall, f1 for each class)\n",
    "\n",
    "7. **Print Evaluation Metrics**  \n",
    "   Displays accuracy and F1 score on the console.\n",
    "\n",
    "8. **Predict on Custom Reviews**  \n",
    "   Tests the trained model on three custom example sentences.  \n",
    "   For each, it predicts sentiment and prints:  \n",
    "   - The review snippet  \n",
    "   - Predicted sentiment (Positive/Negative)  \n",
    "   - Confidence percentage of the prediction\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "This demo function shows how to:\n",
    "\n",
    "- Load and prepare data for sentiment classification  \n",
    "- Fine-tune a BERT model on a sample dataset  \n",
    "- Evaluate the model’s performance  \n",
    "- Make predictions on new, unseen text  \n",
    "\n",
    "It’s a full mini pipeline demonstrating text classification with BERT.\n",
    "\n",
    "---\n",
    "\n",
    "> **Note:**  \n",
    "> This is intended as a small-scale demo (using only 1000 samples and 1 epoch) for quick experimentation and learning, not for production-ready accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBxYZ0B7VTNX"
   },
   "outputs": [],
   "source": [
    "def run_text_classification_demo():\n",
    "\n",
    "    \"\"\"Demo for text classification\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TEXT CLASSIFICATION (Sentiment Analysis) DEMO\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    classifier = BERTTextClassifier(num_classes=2)\n",
    "\n",
    "    # Load data\n",
    "    train_texts, train_labels, test_texts, test_labels = classifier.load_imdb_data(sample_size=1000)\n",
    "\n",
    "    # Show sample\n",
    "    print(f\"\\nSample Review: {train_texts[0][:200]}...\")\n",
    "    print(f\"Label: {'Positive' if train_labels[0] == 1 else 'Negative'}\")\n",
    "\n",
    "    # Train for 2 epochs (small for demo)\n",
    "    classifier.train(train_texts, train_labels, epochs=1, batch_size=8)\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy, f1, report = classifier.evaluate(test_texts, test_labels, batch_size=8)\n",
    "\n",
    "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Test custom examples\n",
    "    custom_reviews = [\n",
    "        \"This movie was fantastic! Amazing acting and great plot.\",\n",
    "        \"Boring and terrible. Waste of time.\",\n",
    "        \"Not bad, could be better though.\"\n",
    "    ]\n",
    "\n",
    "    predictions, probabilities = classifier.predict(custom_reviews)\n",
    "\n",
    "    print(f\"\\nCustom Predictions:\")\n",
    "\n",
    "    for text, pred, prob in zip(custom_reviews, predictions, probabilities):\n",
    "\n",
    "        sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
    "\n",
    "        confidence = prob[pred] * 100\n",
    "\n",
    "        print(f\"'{text[:50]}...' -> {sentiment} ({confidence:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of `run_ner_demo()` Function\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This function runs a **demo for Named Entity Recognition (NER)** using a BERT-based model.  \n",
    "It demonstrates loading data, training, evaluating, and predicting named entities in text.\n",
    "\n",
    "---\n",
    "\n",
    "### Workflow Breakdown\n",
    "\n",
    "1. **Print Demo Header**  \n",
    "   Prints a decorative header announcing the NER demo in the console.\n",
    "\n",
    "2. **Initialize NER Model**  \n",
    "   Creates an instance of `BERTNERClassifier` configured for 9 NER labels (entity types).\n",
    "\n",
    "3. **Load Dataset Subset**  \n",
    "   Calls `load_wikiann_data()` on the model to load a small subset (500 samples) of the WikiAnn dataset for quick training/testing:  \n",
    "   - `train_tokens`: tokenized training sentences  \n",
    "   - `train_labels`: corresponding NER labels for each token  \n",
    "   - `test_tokens`: tokenized test sentences  \n",
    "   - `test_labels`: corresponding NER labels for test tokens  \n",
    "\n",
    "4. **Display Sample Tokens and Labels**  \n",
    "   Prints the first 10 tokens of the first training sample.  \n",
    "   Converts numeric labels into human-readable label names (e.g., `B-PER`, `I-LOC`) and prints them.\n",
    "\n",
    "5. **Train the Model**  \n",
    "   Fine-tunes the NER model for 1 epoch on the training tokens and labels, with batch size 8.\n",
    "\n",
    "6. **Evaluate the Model**  \n",
    "   Evaluates the model on the test data to calculate:  \n",
    "   - Accuracy (correct token label predictions percentage)  \n",
    "   - F1 Score (harmonic mean of precision and recall for entity recognition)\n",
    "\n",
    "7. **Print Evaluation Metrics**  \n",
    "   Prints the accuracy and F1 score for the test set.\n",
    "\n",
    "8. **Predict on Custom Sentences**  \n",
    "   Provides two sample sentences as token lists and runs the model’s `predict()` method.  \n",
    "   Prints tokens alongside their predicted NER labels.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "This function demonstrates a full NER pipeline, including:\n",
    "\n",
    "- Loading and preparing token-level NER data  \n",
    "- Fine-tuning a BERT-based NER classifier  \n",
    "- Evaluating model performance  \n",
    "- Predicting entities on new sentences  \n",
    "\n",
    "It’s designed as a quick, illustrative example using a small data subset.\n",
    "\n",
    "---\n",
    "\n",
    "> **Note:**  \n",
    "> This demo is simplified for speed and learning purposes, so it uses only 500 samples and trains for 1 epoch.  \n",
    "> Real applications typically require more data and longer training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZ3FV7p3Vo7G"
   },
   "outputs": [],
   "source": [
    "def run_ner_demo():\n",
    "\n",
    "    \"\"\"Demo for Named Entity Recognition\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"NAMED ENTITY RECOGNITION DEMO\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    ner_model = BERTNERClassifier(num_labels=9)\n",
    "\n",
    "    # Load small subset for demo (fast training)\n",
    "    train_tokens, train_labels, test_tokens, test_labels = ner_model.load_wikiann_data(sample_size=500)\n",
    "\n",
    "    # Show sample\n",
    "    print(f\"\\nSample tokens: {train_tokens[0][:10]}\")\n",
    "\n",
    "    label_names = [ner_model.labels[l] if l < len(ner_model.labels) else \"O\" for l in train_labels[0][:10]]\n",
    "\n",
    "    print(f\"Sample labels: {label_names}\")\n",
    "\n",
    "    # Train for 2 epochs\n",
    "    ner_model.train(train_tokens, train_labels, epochs=1, batch_size=8)\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy, f1 = ner_model.evaluate(test_tokens, test_labels, batch_size=8)\n",
    "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Custom examples\n",
    "    custom_sentences = [\n",
    "        [\"John\", \"Smith\", \"works\", \"at\", \"Google\", \"in\", \"California\"],\n",
    "        [\"Apple\", \"Inc.\", \"was\", \"founded\", \"by\", \"Steve\", \"Jobs\"]\n",
    "    ]\n",
    "\n",
    "    predictions = ner_model.predict(custom_sentences)\n",
    "    print(f\"\\nCustom Predictions:\")\n",
    "    for tokens, preds in zip(custom_sentences, predictions):\n",
    "        print(\"Tokens:\", tokens)\n",
    "        print(\"Labels:\", preds)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of `run_qa_demo()` Function\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This function runs a **demo for Question Answering (QA)** using a BERT-based QA model.  \n",
    "It demonstrates how to load data, train the model, and answer questions based on a given context.\n",
    "\n",
    "---\n",
    "\n",
    "### Workflow Breakdown\n",
    "\n",
    "1. **Print Demo Header**  \n",
    "   Prints a visually clear header to announce the start of the Question Answering demo.\n",
    "\n",
    "2. **Initialize QA Model**  \n",
    "   Creates an instance of `BERTQuestionAnswering`, which internally loads a BERT model and tokenizer for QA tasks.\n",
    "\n",
    "3. **Load SQuAD Dataset Subset**  \n",
    "   Uses the `load_squad_data()` method to load a small subset (500 samples) of the SQuAD dataset, which contains:  \n",
    "   - `train_questions`: list of questions in the training set  \n",
    "   - `train_contexts`: corresponding paragraphs (contexts)  \n",
    "   - `train_answers`: ground truth answers with start positions  \n",
    "   - `val_questions`, `val_contexts`, `val_answers`: similarly for validation set  \n",
    "\n",
    "4. **Print Sample Data**  \n",
    "   Displays a sample question, the first 200 characters of the corresponding context, and the ground truth answer (or a fallback message if missing).\n",
    "\n",
    "5. **Train the Model**  \n",
    "   Calls the `train()` method to fine-tune the BERT QA model on the training data for 1 epoch (batch size 4) — a small number for demo purposes.\n",
    "\n",
    "6. **Run Custom Q&A Tests**  \n",
    "   Defines a list of custom question-context pairs not from the dataset.  \n",
    "   For each pair, it calls the `answer_question()` method of the model to predict answers and prints the question and the predicted answer.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "This demo shows a complete pipeline for:\n",
    "\n",
    "- Loading a QA dataset (SQuAD subset)  \n",
    "- Preparing and training a BERT QA model  \n",
    "- Testing the model with new, unseen questions and contexts  \n",
    "- Outputting the predicted answers for human review  \n",
    "\n",
    "---\n",
    "\n",
    "### Notes\n",
    "\n",
    "- The training sample size and epochs are kept small for quick demonstration.  \n",
    "- The `answer_question()` method limits the answer span length (max 30 tokens) to avoid excessively long predictions.  \n",
    "- This pipeline can be extended to larger datasets, longer training, and different QA models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8Fx5D5uVpki"
   },
   "outputs": [],
   "source": [
    "def run_qa_demo():\n",
    "\n",
    "    \"\"\"Demo for Question Answering\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"QUESTION ANSWERING DEMO\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    qa_model = BERTQuestionAnswering()\n",
    "\n",
    "    # Load small subset (for speed)\n",
    "    (train_questions, train_contexts, train_answers,\n",
    "     val_questions, val_contexts, val_answers) = qa_model.load_squad_data(sample_size=500)\n",
    "\n",
    "    # Safe sample print\n",
    "    ans_text = train_answers[0]['text'][0] if train_answers[0]['text'] else 'No answer'\n",
    "\n",
    "    print(f\"\\nSample Question: {train_questions[0]}\")\n",
    "\n",
    "    print(f\"Sample Context: {train_contexts[0][:200]}...\")\n",
    "\n",
    "    print(f\"Sample Answer: {ans_text}\")\n",
    "\n",
    "    # Train model (2 epochs for demo)\n",
    "    qa_model.train(train_questions, train_contexts, train_answers, epochs=1, batch_size=4)\n",
    "\n",
    "    # Test on custom questions\n",
    "    print(f\"\\nCustom Q&A Examples:\")\n",
    "\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"question\": \"What is the capital of France?\",\n",
    "            \"context\": \"France is a country in Europe. Paris is the capital and largest city of France. The city is known for the Eiffel Tower and the Louvre Museum.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Who founded Apple?\",\n",
    "            \"context\": \"Apple Inc. is an American technology company. It was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in 1976. The company is known for products like iPhone and Mac.\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    for case in test_cases:\n",
    "        answer = qa_model.answer_question(case[\"question\"], case[\"context\"], max_answer_len=30)\n",
    "        print(f\"Q: {case['question']}\")\n",
    "        print(f\"A: {answer}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Multi-Task Demo Launcher Explanation\n",
    "\n",
    "## Purpose  \n",
    "This script provides a simple command-line menu for users to select and run demos of different BERT-based NLP tasks:\n",
    "- Text Classification (Sentiment Analysis)\n",
    "- Named Entity Recognition (NER)\n",
    "- Question Answering (QA)\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Display Menu**  \n",
    "   The script prints a header and a list of task options for the user to choose from.\n",
    "\n",
    "2. **User Input**  \n",
    "   It prompts the user to enter a choice (1 to 4), then strips any extra spaces.\n",
    "\n",
    "3. **Execute Task(s)**  \n",
    "   Based on the input:\n",
    "   - **1:** Runs the Text Classification demo.\n",
    "   - **2:** Runs the NER demo.\n",
    "   - **3:** Runs the Question Answering demo.\n",
    "   - **4:** Runs all three demos sequentially.\n",
    "   - **Invalid input:** Shows an error message indicating the choice is invalid.\n",
    "\n",
    "4. **Error Handling**  \n",
    "   Wraps the task execution inside a try-except block to catch any runtime errors.  \n",
    "   If an error occurs, it displays:\n",
    "   - The error message.\n",
    "   - Instructions to ensure necessary packages and classes are installed and loaded.\n",
    "   - Advice on using fixed versions of the code to avoid common bugs.\n",
    "\n",
    "## Key Points\n",
    "\n",
    "- **User-Friendly Interface:** Provides a clean and straightforward way to run different BERT demos without modifying the code.  \n",
    "- **Robustness:** Includes error handling to inform the user about missing dependencies or other issues.  \n",
    "- **Dependencies:** Requires that all necessary Python packages and demo classes/datasets are installed and available.  \n",
    "- **Flexibility:** Allows running individual tasks or all tasks in one go for demonstration or testing purposes.\n",
    "\n",
    "## Summary  \n",
    "This script is a launcher that helps users quickly demo multiple NLP tasks powered by BERT models via a simple menu system, with built-in error handling and user guidance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 769
    },
    "id": "q1s2cTD6VzOe",
    "outputId": "23ec3195-9aac-4370-cf0e-8eb55f99a9cb"
   },
   "outputs": [],
   "source": [
    "print(\"BERT Multi-Task Demo\")\n",
    "print(\"Choose a task to run:\")\n",
    "print(\"1. Text Classification (Sentiment Analysis)\")\n",
    "print(\"2. Named Entity Recognition (NER)\")\n",
    "print(\"3. Question Answering\")\n",
    "print(\"4. Run All Tasks\")\n",
    "\n",
    "choice = input(\"\\nEnter your choice (1-4): \").strip()\n",
    "\n",
    "try:\n",
    "    if choice == \"1\":\n",
    "        run_text_classification_demo()\n",
    "    elif choice == \"2\":\n",
    "        run_ner_demo()\n",
    "    elif choice == \"3\":\n",
    "        run_qa_demo()\n",
    "    elif choice == \"4\":\n",
    "        run_text_classification_demo()\n",
    "        run_ner_demo()\n",
    "        run_qa_demo()\n",
    "    else:\n",
    "        print(\"Invalid choice! Please run again.\")\n",
    "except Exception as e:\n",
    "    print(\"\\n--- ERROR OCCURRED ---\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nMake sure you have:\")\n",
    "    print(\"1. Installed required packages:\")\n",
    "    print(\"   pip install torch transformers datasets scikit-learn tqdm numpy\")\n",
    "    print(\"2. Loaded all classes & dataset helpers (BERTTextClassifier, BERTNERClassifier, BERTQuestionAnswering, TextClassificationDataset, NERDataset, QADataset)\")\n",
    "    print(\"3. Using the fixed versions (with int casting, offset_mapping for QA, word_ids fix for NER)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfXDa53caeZO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
