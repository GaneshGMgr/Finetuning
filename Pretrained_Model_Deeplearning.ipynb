{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine Tuning with Mistral, QLora, and PEFt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ### ResNet-50 Image Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook demonstrates the process of fine-tuning a language model using Mistral with QLora and PEFt enhancements. We will go through the setup, configuration, and execution of the fine-tuning process, and evaluate the performance of the fine-tuned model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the pretrained ResNet-50 model (ImageNet weight)\n",
        "model = ResNet50(weights='imagenet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine-tuning Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path to the image you want to classify\n",
        "img_path = '/content/dog.jpeg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the image and convert it to the format expected by ResNet-50\n",
        "img = image.load_img(img_path, target_size=(224, 224))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0) # add a batch dimension\n",
        "x = preprocess_input(x) # apply model-specific preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make a prediction\n",
        "preds = model.predict(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decode the top-5 predictions\n",
        "for _, label, prob in decode_predictions(preds, top=10)[0]:\n",
        "    print(f\"{label}: {prob:.4f}\")  # fixed typo: lable -> label and corrected indentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### BERT for Masked Language Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Transformer if not install already\n",
        "# !pip install --upgrade transformers torch --quiet\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load tokenizer + pre_trained BERT (MLMhead included)\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# input text (use [MASK] where we want BERT to predict a token)\n",
        "text = \"The capital city of France is [MASK].\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess (tokenize + convert to tensors)\n",
        "inputs = tokenizer(text, return_tensors=\"pt\") # batch of size 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Forward pass (no gradients needed for inference)\n",
        "with torch.inference_mode():\n",
        "    logits = model(**inputs).logits # shape: [batch, seq_len, vocab]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the position of the [MASK] token\n",
        "mask_idx = (inputs.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Soft-max over vocab + take top-k predictions for that position\n",
        "probs = F.softmax(logits[0, mask_idx], dim=-1) # [1, vocab]\n",
        "top_k = torch.topk(probs, k=5, dim=-1) # top-5 guesses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decode and print\n",
        "print(\"Top-5 predictions for [MASK]:\")\n",
        "for token_id, prob in zip(top_k.indices[0], top_k.values[0]):\n",
        "    token = tokenizer.decode([token_id])  # fixed: docode -> decode and corrected brackets\n",
        "    print(f\"{token:<12} -> {prob.item():.4f}\")  # fixed: bracket mismatch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  BERT Sentiment Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model name: BERT fine-tuned for classification\n",
        "model_name = \"textattack/bert-base-uncased-SST-2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load manually\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict\n",
        "result = classifier(\"The movie was masterpiece. I loved it!\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenizer Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "tok = BertTokenizer.from_pretrained(\"bert-base-uncased\")  # fixed: \"bert-base_uncased\" -> \"bert-base-uncased\"\n",
        "output = tok(\"I love you\", return_tensors=\"pt\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### BERT Tokenizer with GPT2 Model (Mismatch Warning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text->Token->Embedding->Model input\n",
        "from transformers import BertTokenizer, GPT2Model\n",
        "tok = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = GPT2Model.from_pretrained(\"gpt2\")\n",
        "ids = tok(\"hello world\", return_tensors=\"pt\")[\"input_ids\"]\n",
        "model(ids)  # RuntimeError: size mismatch in Embedding layer"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
