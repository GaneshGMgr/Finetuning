{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10a38426",
   "metadata": {},
   "source": [
    "## Why_finetuning_was_not_possible_in_LSTM.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037071d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d203e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 200\n",
    "vocab_size = 10000\n",
    "embedding_dim = 128\n",
    "latent_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be05360",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), _ = tf.keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "len(x_train)\n",
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092052f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[:3000]\n",
    "y_train = y_train[:3000]\n",
    "\n",
    "len(x_train)\n",
    "len(y_train)\n",
    "\n",
    "len(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344f2e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_sequences(x_train, maxlen=max_len, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b96f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(max_len,))\n",
    "embedding_layer = Embedding(vocab_size, embedding_dim)(input_layer)\n",
    "lstm_layer, state_h, state_c = LSTM(latent_dim, return_state=True)(embedding_layer)\n",
    "output_layer = Dense(1, activation='sigmoid')(state_h)\n",
    "classification_model = Model(input_layer, output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152417ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model.layers\n",
    "classification_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2f098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "classification_model.fit(x_train, y_train, batch_size=64, epochs=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ea6168",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_review = x_train[0].reshape(1, -1)\n",
    "prediction = classification_model.predict(sample_review)\n",
    "\n",
    "print(\"Predicted sentiment probability (positive):\", prediction[0][0])\n",
    "print(\"Predicted Sentiment: \", \"Positive\" if prediction[0][0] > 0.5 else \"Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df212e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = {index + 3: word for word, index in word_index.items()}\n",
    "\n",
    "reverse_word_index[0] = \"\"\n",
    "reverse_word_index[1] = \"\"\n",
    "reverse_word_index[2] = \"\"\n",
    "reverse_word_index[3] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b1494",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_review = \"\".join([reverse_word_index.get(i, \"\") for i in sample_review[0]])\n",
    "print(\"****Decoded Review:****\")\n",
    "print(decoded_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8b3169",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model.save(\"lstm_imdb_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4cd381",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 200\n",
    "vocab_size = 1000\n",
    "embedding_dim = 128\n",
    "latent_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6f79ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), _ = tf.keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "x_train = pad_sequences(x_train, maxlen=max_len, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc48664",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[:1000]\n",
    "y_train = y_train[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27f8abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_classification_model = load_model(\"lstm_imdb_model.h5\")\n",
    "load_classification_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "load_classification_model.fit(x_train, y_train, batch_size=64, epochs=1, validation_split=0.1)\n",
    "load_classification_model.save(\"lstm_imdb_model_updated.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b266ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_classification_model = load_model(\"lstm_imdb_model_updated.h5\")\n",
    "\n",
    "encoder_inputs = Input(shape=(max_len,))\n",
    "encoder_embedding = updated_classification_model.layers[1](encoder_inputs)\n",
    "encoder_outputs, state_h, state_c = updated_classification_model.layers[2](encoder_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce652519",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_vocab_size = 8000\n",
    "target_max_len = 50\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding = Embedding(output_vocab_size, embedding_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])\n",
    "decoder_dense = Dense(output_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714bb3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "seq2seq_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320eb2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = x_train[:1000]\n",
    "decoder_input_data = np.random.randint(1, output_vocab_size, (1000, target_max_len))\n",
    "decoder_target_data = np.random.randint(1, output_vocab_size, (1000, target_max_len, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4cdd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data\n",
    "decoder_input_data\n",
    "decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1acf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=32,\n",
    "    epochs=1,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
