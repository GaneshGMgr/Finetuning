{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunnysavita10/Complete-LLM-Finetuning/blob/main/Knowledge_DIstillation_in_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz9u6pZNjZNZ"
      },
      "source": [
        "# Classical LLM Distillation (BERT â†’ DistilBERT)\n",
        "\n",
        "> **Goal:** Train a small **student** model to learn from a large **teacher** model by combining *soft* predictions (teacher outputs) with *hard* labels (ground truth).\n",
        "\n",
        "## ðŸ“š References\n",
        "- [Hinton et al., *Distilling the Knowledge in a Neural Network* (2015)](https://arxiv.org/pdf/1503.02531)  \n",
        "- [*Knowledge Distillation: A Survey*](https://arxiv.org/pdf/2006.05525)  \n",
        "- [*DistilBERT: smaller, faster, cheaper and lighter*](https://arxiv.org/pdf/1910.01108)  \n",
        "- [*TinyStories: How Small Can Language Models Be and Still Speak Coherent English?*](https://arxiv.org/pdf/2305.07759)  \n",
        "- [*Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes*](https://arxiv.org/pdf/2305.02301)  \n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "This is a step-by-step outline of how Large Language Model (LLM) distillation is performed, using **BERT** as the teacher model and **DistilBERT** as the student model.\n",
        "\n",
        "---\n",
        "\n",
        "## Process Breakdown\n",
        "\n",
        "| Block                   | Purpose                                                                                                                              |\n",
        "| ----------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |\n",
        "| **0. Imports & Config** | Import all required Python libraries (Transformers, Torch, etc.), detect GPU/CPU, and set hyperparameters (batch size, learning rate, epochs, temperature, alpha). |\n",
        "| **1. Dataset**          | Load a demo text dataset, tokenize it using the tokenizer; the data collator automatically pads sequences so each batch is uniform. |\n",
        "| **2. Models**           | Load pretrained **BERT** as the teacher (freeze parameters), and **DistilBERT** as the student (trainable).                         |\n",
        "| **3. Losses**           | Use two loss functions: <br>â€¢ **CrossEntropy (CE)** = for hard labels (ground truth) <br>â€¢ **KL Divergence** = for soft labels (teacherâ€™s logits â†’ softmax with temperature). |\n",
        "| **4. Optimizer**        | Use **AdamW** optimizer with a **linear learning rate scheduler** for smoother and more stable training.                            |\n",
        "| **5. distill_epoch()**  | For each batch: <br>â€¢ Get teacher logits and create soft targets with temperature <br>â€¢ Get student logits <br>â€¢ Compute soft loss and hard loss, combine them using Î± <br>â€¢ Backpropagate **only** through the student model. |\n",
        "| **evaluate()**          | Evaluate the student modelâ€™s accuracy on the validation set to monitor performance improvements.                                   |\n",
        "| **Loop**                | For each epoch: run `distill_epoch()` followed by `evaluate()`.                                                                     |\n",
        "| **Save**                | Save the fine-tuned student model to disk for future inference.                                                                     |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic7PI1D4Rn3h"
      },
      "source": [
        "# Distillation: step-by-step (concise reference)\n",
        "\n",
        "> **Tip:** For very large datasets or multi-GPU/TPU training use Hugging Faceâ€™s `DistillationTrainer` or `accelerate`. The core algorithm below stays the same.\n",
        "\n",
        "---\n",
        "\n",
        "## Overview (whatâ€™s happening)\n",
        "1. Train a large **teacher** model (high capacity) normally and freeze it.  \n",
        "2. Train a smaller **student** model to mimic the teacher **and** the ground-truth labels.  \n",
        "3. Student loss = weighted combination of a **soft** (teacher) loss and a **hard** (label) loss.\n",
        "\n",
        "---\n",
        "\n",
        "## Step-by-step\n",
        "\n",
        "### 1. Teacher output (`t_soft`)\n",
        "- Compute teacher logits: `z_T = teacher(x)` (teacher is frozen; run under `torch.no_grad()`).\n",
        "- Apply **temperature** `T` and softmax to get *soft targets*:\n",
        "  \\[\n",
        "  p_T = \\text{softmax}\\!\\left(\\frac{z_T}{T}\\right)\n",
        "  \\]\n",
        "- `T > 1` â€œsoftensâ€ the distribution (reveals class similarities).\n",
        "\n",
        "### 2. Student output (`s_soft`)\n",
        "- Compute student logits: `z_S = student(x)`.\n",
        "- Convert to log-probabilities at the same temperature:\n",
        "  \\[\n",
        "  \\log q_S = \\log\\text{softmax}\\!\\left(\\frac{z_S}{T}\\right)\n",
        "  \\]\n",
        "\n",
        "### 3. Distillation (soft) loss\n",
        "- Use KL divergence (teacher distribution â†’ student distribution).\n",
        "- In PyTorch style: `nn.KLDivLoss(reduction='batchmean')(log_q_S, p_T)`\n",
        "- Multiply the KL loss by `T^2` to correct gradient scale (Hinton et al.):\n",
        "  \\[\n",
        "  L_{\\text{soft}} = T^2 \\cdot \\text{KL}(p_T \\,\\|\\, q_S)\n",
        "  \\]\n",
        "\n",
        "### 4. Hard (label) loss\n",
        "- Standard cross-entropy between student logits and true labels:\n",
        "  \\[\n",
        "  L_{\\text{hard}} = \\text{CE}(z_S, y)\n",
        "  \\]\n",
        "\n",
        "### 5. Combine\n",
        "- Weighted sum:\n",
        "  \\[\n",
        "  L = \\alpha \\cdot L_{\\text{soft}} + (1-\\alpha)\\cdot L_{\\text{hard}}\n",
        "  \\]\n",
        "- Typical choices: `T âˆˆ [2,5]`, `Î± â‰ˆ 0.5` (tune for your task).\n",
        "- If `Î± = 1` â†’ pure distillation (no hard labels). If `Î± = 0` â†’ normal fine-tuning (no distillation).\n",
        "\n",
        "---\n",
        "\n",
        "## Implementation notes / best practices\n",
        "- Freeze teacher: `teacher.eval()` and use `with torch.no_grad()` when generating `z_T`. This saves memory and avoids updating teacher weights.\n",
        "- Use `F.softmax(z_T / T, dim=-1)` for teacher targets and `F.log_softmax(z_S / T, dim=-1)` for student input to `KLDivLoss`.\n",
        "- In PyTorch, prefer `nn.KLDivLoss(reduction='batchmean')` for stable gradients.\n",
        "- Multiply KL term by `T**2` (important â€” otherwise gradients from soft targets are scaled down).\n",
        "- Monitor both components (`L_soft`, `L_hard`) and validation accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "## Small numeric example (temperature effect)\n",
        "- Teacher logits: `[10, 2]`\n",
        "  - `T = 1` â†’ `softmax([10,2]) â‰ˆ [0.9997, 0.0003]` (very peaked)\n",
        "  - `T = 2` â†’ `softmax([5,1]) â‰ˆ [0.982, 0.018]` (softer, reveals second choice)\n",
        "  - `T = 5` â†’ `softmax([2,0.4]) â‰ˆ [0.83, 0.17]` (much softer)\n",
        "- Softer distributions reveal the teacherâ€™s relative beliefs and help the student learn nuanced class relations.\n",
        "\n",
        "---\n",
        "\n",
        "## Why distillation helps\n",
        "- **Soft targets** encode â€œdark knowledgeâ€: relative similarities between classes that hard labels hide.  \n",
        "- Student learns both the dataset labels **and** the teacherâ€™s nuanced behavior â†’ better generalization for a much smaller model.\n",
        "\n",
        "---\n",
        "\n",
        "## Short pseudocode (conceptual)\n",
        "1. `z_T = teacher(x)`  (no grad)\n",
        "2. `p_T = softmax(z_T / T)`\n",
        "3. `z_S = student(x)`\n",
        "4. `log_q_S = log_softmax(z_S / T)`\n",
        "5. `loss_soft = T^2 * KLDiv(log_q_S, p_T)`\n",
        "6. `loss_hard = CrossEntropy(z_S, y)`\n",
        "7. `loss = alpha * loss_soft + (1 - alpha) * loss_hard`\n",
        "8. `loss.backward()` and `optimizer.step()` (update only student)\n",
        "\n",
        "---\n",
        "\n",
        "## Quick hyperparameter suggestions\n",
        "- `T = 2` (good starting point), try `2â€“5`.  \n",
        "- `alpha = 0.3â€“0.7` depending on trust in teacher vs labels.  \n",
        "- `batch size`: 64â€“256 (task-dependent).  \n",
        "- Ensure teacher has good accuracy before distillation.\n",
        "\n",
        "---\n",
        "\n",
        "## Final note\n",
        "Distillation is **not** just fine-tuning: it explicitly transfers the teacherâ€™s learned distributional knowledge (soft targets) into a compact student while still respecting hard labels. For large-scale runs, use `DistillationTrainer` / `accelerate` to scale cleanly across devices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sdjDT-ERfra",
        "outputId": "d636c730-48da-495e-f6ce-647fb34523d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2025.3.0)\n",
            "Collecting fsspec\n",
            "  Using cached fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.34.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# !pip install --upgrade datasets fsspec transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xDFVdc-H7Y7u"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "U1zai2NZxr79"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YsmgAOVS2ckB"
      },
      "outputs": [],
      "source": [
        "batch_size   = 16\n",
        "lr           = 5e-5\n",
        "epochs       = 1\n",
        "temperature  = 2.0\n",
        "alpha_soft   = 0.5\n",
        "max_len      = 128\n",
        "device       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "69ip12Nz2evJ"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "raw = load_dataset(\"tweet_eval\", \"sentiment\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kCphqBQ_x5q0"
      },
      "outputs": [],
      "source": [
        "label_feature = raw[\"train\"].features[\"label\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mL2TDIQ3-RrC",
        "outputId": "39aed96b-93bb-47c7-b297-fd9678a7c146"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label names: ['negative', 'neutral', 'positive']\n"
          ]
        }
      ],
      "source": [
        "print(\"Label names:\", label_feature.names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "O8h_vsYox8hr"
      },
      "outputs": [],
      "source": [
        "# Subset (2.5k samples for train)\n",
        "train = raw['train'].shuffle(seed=42).select(range(2500))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rK8MRNdHyfeL"
      },
      "source": [
        "Why Train is Subset, but Val is Not\n",
        "\n",
        "ðŸ”¹ 1. Training cost is high, validation cost is low\n",
        "\n",
        "Training = multiple forward + backward passes â†’ GPU heavy\n",
        "\n",
        "Validation = only forward pass, no gradient update â†’ fast\n",
        "\n",
        "So itâ€™s common to reduce training size for quick experiments but keep full validation for accurate metric evaluation.\n",
        "\n",
        "ðŸ”¹ 2. Keeping val full improves generalization check\n",
        "\n",
        "If you also reduce validation (e.g., from 872 â†’ 100), metrics become noisy and unreliable.\n",
        "\n",
        "Full validation gives stable accuracy/loss during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "R344cVlw2kiu"
      },
      "outputs": [],
      "source": [
        "val   = raw['validation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pZ7DWOo22oqM"
      },
      "outputs": [],
      "source": [
        "#Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "343oz61F2rYz"
      },
      "outputs": [],
      "source": [
        "def tokenize(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, max_length=max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5GiRzIbhynnV"
      },
      "outputs": [],
      "source": [
        "# Tokenize & remove original text column\n",
        "tokenized = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b38abe0848384e629c89bfff7ed2c880",
            "105b185aaa804a7aba4a1c441f64c11f",
            "5bf295acb8ab4517aae191a49681cadc",
            "8c027b904a124d2badd7b308127142dc",
            "708f3c5f9e1446d4be67ce89bf4a08d7",
            "5029c89c804143efa74cb4bf95f4f379",
            "77ff366eae15414d926c4647a62712ca",
            "65f08a7f4a18473eaf8d80b7c3ef013e",
            "f860cef2f6554f27ac59e2aa06b33872",
            "8a3b79955c5c4841b599811df227789e",
            "d7e4e24960be4bb4862d3529055c7d39"
          ]
        },
        "id": "E5IsoQQ327B3",
        "outputId": "fd2d5b4e-f310-4b1d-a5f4-cbfa89f419e6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b38abe0848384e629c89bfff7ed2c880",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized['train'] = train.map(tokenize, batched=True, remove_columns=['text'])\n",
        "tokenized['validation'] = val.map(tokenize, batched=True, remove_columns=['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6j65Youl2-VV"
      },
      "outputs": [],
      "source": [
        "# Data Collator (auto-padding)\n",
        "collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_xx7gIvLy9JZ"
      },
      "outputs": [],
      "source": [
        "# DataLoaders\n",
        "train_dl = DataLoader(tokenized['train'], batch_size=batch_size,shuffle=True, collate_fn=collator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "x_DfUjRhSw_u"
      },
      "outputs": [],
      "source": [
        "val_dl = DataLoader(tokenized['validation'], batch_size=batch_size,shuffle=False, collate_fn=collator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "G42tryte3VX0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "vOPWt6vb9nnv"
      },
      "outputs": [],
      "source": [
        "num_labels = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qi44rdyBR96l",
        "outputId": "29398d6b-59de-418e-d148-a192b0f4c719"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "teacher = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-large-uncased\", num_labels=num_labels).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFphv9J-SDwA",
        "outputId": "305e7b76-beb2-4458-f865-47e137b033ff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "student = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", num_labels=num_labels).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxcJJBxaSDze",
        "outputId": "e415a1b5-9dd0-455b-beec-0665946c4899"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 1024)\n",
              "      (token_type_embeddings): Embedding(2, 1024)\n",
              "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-23): 24 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=1024, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Freeze teacher (no training): We â€œlockâ€ the teacher so it doesnâ€™t learn anymore and ensure it behaves predictably while \n",
        "# generating soft targets for the student\n",
        "for p in teacher.parameters():\n",
        "    p.requires_grad = False\n",
        "teacher.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "R4c9GrXL0y3S"
      },
      "outputs": [],
      "source": [
        "ce_loss = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "1xTj0BAR00m2"
      },
      "outputs": [],
      "source": [
        "kl_loss = nn.KLDivLoss(reduction=\"batchmean\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "k2iu8MuB5sh1"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.AdamW(student.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "IwEFLyk_9LQQ"
      },
      "outputs": [],
      "source": [
        "from transformers import get_scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Understanding Learning Rate Decay and Linear Scheduler\n",
        "\n",
        "---\n",
        "\n",
        "## What is Learning Rate Decay?\n",
        "\n",
        "Imagine learning a new skill, like painting:\n",
        "\n",
        "- **At first**, you paint with big brush strokes â€” making big changes quickly (high learning rate).\n",
        "- **As you improve**, you make smaller, more precise strokes to avoid ruining your work (lower learning rate).\n",
        "\n",
        "Learning rate decay is the process of **starting with a high learning rate and gradually reducing it during training** to help the model converge better.\n",
        "\n",
        "---\n",
        "\n",
        "## What Does Linear Decay Mean?\n",
        "\n",
        "- You start with an initial learning rate (e.g., 0.1).\n",
        "- After every training step, the learning rate decreases **by the same small amount**, moving linearly towards zero.\n",
        "- Think of it as **shrinking your brush strokes evenly over time**, from big to tiny.\n",
        "\n",
        "### Visual Example of Linear Decay\n",
        "\n",
        "| Step | Learning Rate | Description                     |\n",
        "|-------|--------------|--------------------------------|\n",
        "| 1     | 0.1          | Big steps to learn quickly      |\n",
        "| 2     | 0.09         | Slightly smaller steps          |\n",
        "| 3     | 0.08         | Even smaller steps              |\n",
        "| ...   | ...          | ...                            |\n",
        "| 10    | 0.0          | Tiny steps for fine-tuning     |\n",
        "\n",
        "---\n",
        "\n",
        "## Other Types of Learning Rate Decay\n",
        "\n",
        "- **Exponential Decay:** Drops quickly at first, then slows down.\n",
        "- **Step Decay:** Keeps steady, then drops sharply in intervals.\n",
        "- **Cosine Decay:** Smooth, wave-like gradual drop.\n",
        "- **Warmup + Decay:** Starts small (warmup), then increases before decaying.\n",
        "\n",
        "---\n",
        "\n",
        "## Why Use Learning Rate Decay?\n",
        "\n",
        "- Prevents the model from **jumping around the solution** due to a high learning rate.\n",
        "- Enables **fast initial learning**, followed by **careful fine-tuning**.\n",
        "- Often leads to **better model convergence and stability**.\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Rate Scheduler Setup (Linear Decay)\n",
        "\n",
        "- **Type:** Linear decay scheduler without warmup.\n",
        "- **Purpose:**  \n",
        "  Gradually reduce the learning rate from the initial value down to zero over the full training duration.\n",
        "- **Parameters:**  \n",
        "  - `optimizer`: The optimizer whose learning rate will be updated.  \n",
        "  - `num_warmup_steps`: 0 (no warmup phase).  \n",
        "  - `num_training_steps`: Total number of training steps, calculated as `batches_per_epoch * epochs`.\n",
        "- **Why use:**  \n",
        "  Helps stabilize training by progressively lowering the learning rate, which can improve convergence.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cV-wcgUBSGdX"
      },
      "outputs": [],
      "source": [
        "# Create a learning rate scheduler that linearly decreases the learning rate from\n",
        "# the initial value to zero over the course of training.\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\",                  # Scheduler type: linear decay\n",
        "    optimizer=optimizer,            # Optimizer whose LR will be scheduled\n",
        "    num_warmup_steps=0,             # No warmup steps, LR starts decaying immediately\n",
        "    num_training_steps=len(train_dl) * epochs  # Total steps = batches per epoch * number of epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "YJMLeUVp9U9N"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of `distill_epoch()` Function\n",
        "\n",
        "The `distill_epoch()` function runs one training epoch of **knowledge distillation**, where a smaller **student model** learns from a larger, pretrained **teacher model** by combining teacher outputs with true labels.\n",
        "\n",
        "- **Student in training mode:** Enables dropout and training behaviors.\n",
        "- **Batch processing:** For each batch:\n",
        "  - Move inputs (`input_ids`, `attention_mask`, `labels`) to the device (CPU/GPU).\n",
        "  - **Teacher forward pass (no grad):** Compute teacher logits, then generate *soft targets* with temperature-scaled softmax.\n",
        "  - **Student forward pass:** Compute student logits and log-softmax with the same temperature.\n",
        "  - **Loss computation:**\n",
        "    - *Soft loss:* KL divergence between student and teacher output distributions, scaled by temperature squared.\n",
        "    - *Hard loss:* Cross-entropy loss between student logits and ground-truth labels.\n",
        "    - Combine losses with weighting factor `alpha_soft`.\n",
        "  - **Optimization:**\n",
        "    - Backpropagate combined loss.\n",
        "    - Update student parameters via optimizer.\n",
        "    - Adjust learning rate via scheduler.\n",
        "- **Progress bar:** Displays training loss dynamically.\n",
        "\n",
        "This process helps the student model learn both the nuanced behavior of the teacher (via soft targets) and the actual labels (hard targets), resulting in a smaller, efficient model approximating the teacherâ€™s performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_BujIB9SGg7"
      },
      "outputs": [],
      "source": [
        "def distill_epoch():\n",
        "    # Set student model to training mode (enable dropout, batch norm updates, etc.)\n",
        "    student.train()\n",
        "    \n",
        "    # Wrap dataloader with tqdm for a progress bar display\n",
        "    pbar = tqdm(train_dl, desc=\"Train\")\n",
        "    \n",
        "    # Iterate over batches in the training dataloader\n",
        "    for batch in pbar:\n",
        "        # Move batch data to the right device (GPU or CPU)\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention = batch[\"attention_mask\"].to(device)\n",
        "        labels    = batch[\"labels\"].to(device)\n",
        "\n",
        "        # ---------------- Teacher forward pass ---------------- #\n",
        "        # Disable gradient calculation for teacher to save memory and computation\n",
        "        with torch.no_grad():\n",
        "            # Get raw logits (pre-softmax scores) from teacher model\n",
        "            t_logits = teacher(input_ids, attention_mask=attention).logits\n",
        "            # Apply softmax with temperature to get soft probability targets\n",
        "            t_soft = torch.softmax(t_logits / temperature, dim=1)\n",
        "\n",
        "        # ---------------- Student forward pass ---------------- #\n",
        "        # Get raw logits from student model (these will be updated via backprop)\n",
        "        s_logits = student(input_ids, attention_mask=attention).logits\n",
        "        # Apply log softmax with same temperature for KL divergence loss calculation\n",
        "        s_soft = torch.log_softmax(s_logits / temperature, dim=1)\n",
        "\n",
        "        # ---------------- Loss calculation ---------------- #\n",
        "        # Calculate soft loss: KL divergence between student and teacher distributions\n",
        "        # Multiply by temperature^2 to properly scale gradients (Hinton et al.)\n",
        "        loss_soft = kl_loss(s_soft, t_soft) * (temperature ** 2)\n",
        "        \n",
        "        # Calculate hard loss: cross-entropy between student logits and true labels\n",
        "        loss_hard = ce_loss(s_logits, labels)\n",
        "        \n",
        "        # Combine losses with weighting factor alpha_soft\n",
        "        # alpha_soft controls importance of soft vs hard loss components\n",
        "        loss = alpha_soft * loss_soft + (1 - alpha_soft) * loss_hard\n",
        "\n",
        "        # ---------------- Backpropagation & optimization ---------------- #\n",
        "        # Zero gradients from previous step\n",
        "        optimizer.zero_grad()\n",
        "        # Backpropagate combined loss through student network only\n",
        "        loss.backward()\n",
        "        # Update student model parameters\n",
        "        optimizer.step()\n",
        "        # Update learning rate scheduler if applicable\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        # Update progress bar with current loss value\n",
        "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `evaluate()` Function Explanation\n",
        "\n",
        "This function evaluates the **student** modelâ€™s accuracy on the validation dataset.\n",
        "\n",
        "- **Set model to evaluation mode:**  \n",
        "  `student.eval()` disables dropout and other training-specific layers for stable inference.\n",
        "\n",
        "- **Initialize counters:**  \n",
        "  `correct` counts correctly predicted samples, and `total` counts total samples processed.\n",
        "\n",
        "- **No gradient calculation:**  \n",
        "  `with torch.no_grad()` reduces memory usage and speeds up inference since no backpropagation is needed.\n",
        "\n",
        "- **Batch-wise evaluation loop:**  \n",
        "  For each batch in the validation dataloader (`val_dl`):  \n",
        "  - Move inputs (`input_ids`, `attention_mask`, `labels`) to the appropriate device (CPU/GPU).  \n",
        "  - Forward pass through the student model to get raw logits.  \n",
        "  - Compute predicted classes by taking the `argmax` over logits.  \n",
        "  - Compare predictions with ground-truth labels and update the count of correct predictions.  \n",
        "  - Increment total samples processed.\n",
        "\n",
        "- **Calculate accuracy:**  \n",
        "  Returns the accuracy as a percentage, rounded to two decimal places:  \n",
        "  \\[\n",
        "  \\text{Accuracy} = \\frac{\\text{correct}}{\\text{total}} \\times 100\n",
        "  \\]\n",
        "\n",
        "This function helps monitor how well the student model performs on unseen data during or after training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdqA2MpaSLNn"
      },
      "outputs": [],
      "source": [
        "def evaluate():\n",
        "    # Set student model to evaluation mode (disables dropout, batchnorm, etc.)\n",
        "    student.eval()\n",
        "    \n",
        "    correct = 0  # Counter for correct predictions\n",
        "    total = 0    # Counter for total samples processed\n",
        "    \n",
        "    # Disable gradient calculations for faster inference and lower memory usage\n",
        "    with torch.no_grad():\n",
        "        # Loop over batches in the validation dataloader\n",
        "        for batch in val_dl:\n",
        "            # Move input ids, attention masks, and labels to the device (GPU or CPU)\n",
        "            ids  = batch[\"input_ids\"].to(device)\n",
        "            attn = batch[\"attention_mask\"].to(device)\n",
        "            lbl  = batch[\"labels\"].to(device)\n",
        "            \n",
        "            # Forward pass: get the logits output by the student model\n",
        "            out = student(ids, attention_mask=attn).logits\n",
        "            \n",
        "            # Get predicted class by taking the index with highest logit value\n",
        "            pred = out.argmax(dim=1)\n",
        "            \n",
        "            # Count how many predictions matched the true labels\n",
        "            correct += (pred == lbl).sum().item()\n",
        "            \n",
        "            # Keep track of total number of samples processed\n",
        "            total += lbl.size(0)\n",
        "    \n",
        "    # Calculate and return accuracy percentage rounded to 2 decimals\n",
        "    return round(correct / total * 100, 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "a4e7e0c4350a448ca8015d14b8c17ea9",
            "df0d001249cd4693b3dd7e61b1cdb21c",
            "707c6de352144b30a445edd786aa0be5",
            "d71dd1d8c66649e6943b4cdbe6e13f4b",
            "4b735c50bb1a473b8d9ff7604c33939b",
            "31877f5c2a3044cea968c0ed1a46aa9b",
            "a1f5e69ece2d40e0b9b784701a1fad4e",
            "97d02b8f51cb4739aa9ce1a8e59441a4",
            "b09050601203426fa2d0ad5ffe49ebab",
            "cc6d0f2d01d84783a4de9be9f44afbe6",
            "0440c704c9804f97ab8689ce6a650308"
          ]
        },
        "id": "QnJRKnQ-SLQh",
        "outputId": "694d7e1c-c9c6-49b0-da50-340d468de8e0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4e7e0c4350a448ca8015d14b8c17ea9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Train:   0%|          | 0/157 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1 | Validation Accuracy: 62.7%\n"
          ]
        }
      ],
      "source": [
        "for ep in range(1, epochs + 1):\n",
        "    distill_epoch()\n",
        "    acc = evaluate()\n",
        "    print(f\"Epoch {ep}/{epochs} | Validation Accuracy: {acc}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAD-Zj5RxuDC",
        "outputId": "049b2270-150c-447a-c7fe-9c8543111f48"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('distilled_student_model/tokenizer_config.json',\n",
              " 'distilled_student_model/special_tokens_map.json',\n",
              " 'distilled_student_model/vocab.txt',\n",
              " 'distilled_student_model/added_tokens.json',\n",
              " 'distilled_student_model/tokenizer.json')"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ---------- 6. Save Student ----------\n",
        "student.save_pretrained(\"distilled_student_model\")\n",
        "tokenizer.save_pretrained(\"distilled_student_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary: `predict_and_evaluate` Function\n",
        "\n",
        "- **Purpose:**  \n",
        "  Evaluate a trained model's accuracy on a test dataset and measure inference time.\n",
        "\n",
        "- **Key steps:**  \n",
        "  1. Sets the model to evaluation mode (`model.eval()`) to disable training-specific layers like dropout.  \n",
        "  2. Iterates over the test dataset without computing gradients for efficiency.  \n",
        "  3. Collects predicted labels and true labels for all test samples.  \n",
        "  4. Computes accuracy using `sklearn.metrics.accuracy_score`.  \n",
        "  5. Measures total inference time and calculates average time per sample.  \n",
        "  6. Prints and returns accuracy, total inference time, and average per-sample inference time.\n",
        "\n",
        "- **Usage:**  \n",
        "  Useful for benchmarking model performance and speed on unseen data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khH6Ls1a6zTC"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "\n",
        "def predict_and_evaluate(model, name, test_dl):\n",
        "    # Set the model to evaluation mode (disables dropout, batchnorm, etc.)\n",
        "    model.eval()\n",
        "    \n",
        "    all_preds, all_labels = [], []  # Lists to store predictions and true labels\n",
        "    start_time = time.time()        # Record start time to measure inference duration\n",
        "\n",
        "    # Disable gradient calculations to speed up inference and save memory\n",
        "    with torch.no_grad():\n",
        "        # Loop through batches in the test dataloader\n",
        "        for batch in test_dl:\n",
        "            # Move inputs and labels to the correct device (CPU/GPU)\n",
        "            ids = batch[\"input_ids\"].to(device)\n",
        "            attn = batch[\"attention_mask\"].to(device)\n",
        "            lbls = batch[\"labels\"].to(device)\n",
        "\n",
        "            # Forward pass: get model logits\n",
        "            logits = model(ids, attention_mask=attn).logits\n",
        "            \n",
        "            # Get predicted class indices by taking the max logit along dim=1\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            # Append predictions and true labels to the lists (move to CPU and convert to list)\n",
        "            all_preds.extend(preds.cpu().tolist())\n",
        "            all_labels.extend(lbls.cpu().tolist())\n",
        "\n",
        "    # Calculate total inference time\n",
        "    total_time = time.time() - start_time\n",
        "    \n",
        "    # Calculate accuracy using sklearn's accuracy_score\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    \n",
        "    # Calculate average time per sample (total time divided by total samples)\n",
        "    avg_time = total_time / len(test_dl.dataset)\n",
        "\n",
        "    # Print summary metrics\n",
        "    print(f\"\\n {name}\")\n",
        "    print(f\" Accuracy: {acc*100:.2f}%\")\n",
        "    print(f\" Total Inference Time: {total_time:.2f} sec\")\n",
        "    print(f\" Avg Time per Sample: {avg_time:.4f} sec\")\n",
        "    \n",
        "    # Return accuracy, total inference time, and average time per sample\n",
        "    return acc, total_time, avg_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "xlm2vWAG61M3"
      },
      "outputs": [],
      "source": [
        "# --------- Load test set ---------\n",
        "test = load_dataset(\"tweet_eval\", \"sentiment\", split=\"test[:500]\")  # sample test\n",
        "tokenized_test = test.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
        "tokenized_test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "test_dl = DataLoader(tokenized_test, batch_size=batch_size, shuffle=False, collate_fn=collator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGSKIYvR6oKE",
        "outputId": "d65d7b0e-7bf4-46f2-a043-ea521318a0fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " TEACHER (BERT-Large)\n",
            " Accuracy: 22.00%\n",
            " Total Inference Time: 3.63 sec\n",
            " Avg Time per Sample: 0.0073 sec\n",
            "\n",
            " STUDENT (Distilled BERT)\n",
            " Accuracy: 60.80%\n",
            " Total Inference Time: 1.16 sec\n",
            " Avg Time per Sample: 0.0023 sec\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.608, 1.1580901145935059, 0.0023161802291870115)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# --------- Compare Teacher vs Student ---------\n",
        "predict_and_evaluate(teacher, name=\"TEACHER (BERT-Large)\", test_dl=test_dl)\n",
        "predict_and_evaluate(student, name=\"STUDENT (Distilled BERT)\", test_dl=test_dl)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3b4GLbj8XX9"
      },
      "source": [
        "Backed by Research\n",
        "\n",
        "ðŸ“„ â€œDistilling Step-by-Stepâ€ (Google, ACL 2023)\n",
        "\n",
        "A 770M T5 student outperformed PaLM-540B teacher on multiple tasks using rationale distillation.\n",
        "\n",
        "ðŸ“„ TinyBERT paper (Huawei, 2020)\n",
        "\n",
        "Task-specific distillation allowed TinyBERT to beat BERT-base on SST-2 and MNLI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z99VmmLn8fsn"
      },
      "source": [
        "| Reason                               | Explanation                                                            |\n",
        "| ------------------------------------ | ---------------------------------------------------------------------- |\n",
        "| **TweetEval = small, noisy data** | BERT-Large is overfitting or underconfident due to task size           |\n",
        "| **Student is fine-tuned**         | You updated student weights on TweetEval task                          |\n",
        "| **Teacher is frozen**             | You're using teacher just for soft logits, not re-finetuning           |\n",
        "| **Teacher not task-specific**     | Your BERT-Large is general, but student is task-tuned via distillation |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dojZn3098pJR"
      },
      "source": [
        "| Model   | Accuracy | Speed | Comment                                                 |\n",
        "| ------- | -------- | ----- | ------------------------------------------------------- |\n",
        "| Teacher | 22%      | Slow  | Not tuned, generic, likely overfitting/underfitting     |\n",
        "| Student | 60.8%    | Fast  | Task-specific distilled, learned from soft+hard targets |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOdiUiGfMiuAdrasI4+bGEW",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0440c704c9804f97ab8689ce6a650308": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "071dcdea4d814305805b5924769f7903": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0baf82c3bb6a42a0a178a8eb6fdd4f4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "105b185aaa804a7aba4a1c441f64c11f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5029c89c804143efa74cb4bf95f4f379",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_77ff366eae15414d926c4647a62712ca",
            "value": "Map:â€‡100%"
          }
        },
        "31877f5c2a3044cea968c0ed1a46aa9b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b735c50bb1a473b8d9ff7604c33939b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dfbaf3ac8e84b8dae80028d5f7b2756": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4d5eaaa3b384bd5887a3077f8526d8f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0baf82c3bb6a42a0a178a8eb6fdd4f4a",
            "value": "â€‡2/2â€‡[00:27&lt;00:00,â€‡11.70s/it]"
          }
        },
        "5029c89c804143efa74cb4bf95f4f379": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53230f4538e74920b57473a09898f9ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bf295acb8ab4517aae191a49681cadc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65f08a7f4a18473eaf8d80b7c3ef013e",
            "max": 2000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f860cef2f6554f27ac59e2aa06b33872",
            "value": 2000
          }
        },
        "648579bc6bbe49e1b9b1a783b0dbd212": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65f08a7f4a18473eaf8d80b7c3ef013e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "707c6de352144b30a445edd786aa0be5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97d02b8f51cb4739aa9ce1a8e59441a4",
            "max": 157,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b09050601203426fa2d0ad5ffe49ebab",
            "value": 157
          }
        },
        "708f3c5f9e1446d4be67ce89bf4a08d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7529b8ad8d8240678596d4aa1117eb6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_97b3001459ea46068195ef1a487dbb6a",
              "IPY_MODEL_8ea441ae61af4d8aae04493679a32061",
              "IPY_MODEL_4dfbaf3ac8e84b8dae80028d5f7b2756"
            ],
            "layout": "IPY_MODEL_648579bc6bbe49e1b9b1a783b0dbd212"
          }
        },
        "77ff366eae15414d926c4647a62712ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a760d525e3b443c949724ff5c75bb68": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a3b79955c5c4841b599811df227789e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c027b904a124d2badd7b308127142dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a3b79955c5c4841b599811df227789e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d7e4e24960be4bb4862d3529055c7d39",
            "value": "â€‡2000/2000â€‡[00:00&lt;00:00,â€‡8381.31â€‡examples/s]"
          }
        },
        "8ea441ae61af4d8aae04493679a32061": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53230f4538e74920b57473a09898f9ff",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9dca3fda68304bdab7af54cdf2c30b6f",
            "value": 2
          }
        },
        "97b3001459ea46068195ef1a487dbb6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a760d525e3b443c949724ff5c75bb68",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_071dcdea4d814305805b5924769f7903",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "97d02b8f51cb4739aa9ce1a8e59441a4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dca3fda68304bdab7af54cdf2c30b6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a1f5e69ece2d40e0b9b784701a1fad4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4e7e0c4350a448ca8015d14b8c17ea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df0d001249cd4693b3dd7e61b1cdb21c",
              "IPY_MODEL_707c6de352144b30a445edd786aa0be5",
              "IPY_MODEL_d71dd1d8c66649e6943b4cdbe6e13f4b"
            ],
            "layout": "IPY_MODEL_4b735c50bb1a473b8d9ff7604c33939b"
          }
        },
        "b09050601203426fa2d0ad5ffe49ebab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b38abe0848384e629c89bfff7ed2c880": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_105b185aaa804a7aba4a1c441f64c11f",
              "IPY_MODEL_5bf295acb8ab4517aae191a49681cadc",
              "IPY_MODEL_8c027b904a124d2badd7b308127142dc"
            ],
            "layout": "IPY_MODEL_708f3c5f9e1446d4be67ce89bf4a08d7"
          }
        },
        "cc6d0f2d01d84783a4de9be9f44afbe6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4d5eaaa3b384bd5887a3077f8526d8f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d71dd1d8c66649e6943b4cdbe6e13f4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc6d0f2d01d84783a4de9be9f44afbe6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0440c704c9804f97ab8689ce6a650308",
            "value": "â€‡157/157â€‡[00:44&lt;00:00,â€‡â€‡4.96it/s,â€‡loss=0.4200]"
          }
        },
        "d7e4e24960be4bb4862d3529055c7d39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df0d001249cd4693b3dd7e61b1cdb21c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31877f5c2a3044cea968c0ed1a46aa9b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a1f5e69ece2d40e0b9b784701a1fad4e",
            "value": "Train:â€‡100%"
          }
        },
        "f860cef2f6554f27ac59e2aa06b33872": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
