{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15e25ea",
   "metadata": {},
   "source": [
    "# Classical LLM Distillation (BERT → DistilBERT)\n",
    "\n",
    "## Overview\n",
    "This is a step-by-step outline of how Large Language Model (LLM) distillation is performed, using **BERT** as the teacher model and **DistilBERT** as the student model.\n",
    "\n",
    "---\n",
    "\n",
    "## Process Breakdown\n",
    "\n",
    "| Block                   | Purpose                                                                                                                              |\n",
    "| ----------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| **0. Imports & Config** | Import all required Python libraries (Transformers, Torch, etc.), detect GPU/CPU, and set hyperparameters (batch size, learning rate, epochs, temperature, alpha). |\n",
    "| **1. Dataset**          | Load a demo text dataset, tokenize it using the tokenizer; the data collator automatically pads sequences so each batch is uniform. |\n",
    "| **2. Models**           | Load pretrained **BERT** as the teacher (freeze parameters), and **DistilBERT** as the student (trainable).                         |\n",
    "| **3. Losses**           | Use two loss functions: <br>• **CrossEntropy (CE)** = for hard labels (ground truth) <br>• **KL Divergence** = for soft labels (teacher’s logits → softmax with temperature). |\n",
    "| **4. Optimizer**        | Use **AdamW** optimizer with a **linear learning rate scheduler** for smoother and more stable training.                            |\n",
    "| **5. distill_epoch()**  | For each batch: <br>• Get teacher logits and create soft targets with temperature <br>• Get student logits <br>• Compute soft loss and hard loss, combine them using α <br>• Backpropagate **only** through the student model. |\n",
    "| **evaluate()**          | Evaluate the student model’s accuracy on the validation set to monitor performance improvements.                                   |\n",
    "| **Loop**                | For each epoch: run `distill_epoch()` followed by `evaluate()`.                                                                     |\n",
    "| **Save**                | Save the fine-tuned student model to disk for future inference.                                                                     |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff659dd",
   "metadata": {},
   "source": [
    "# Distillation: step-by-step (concise reference)\n",
    "\n",
    "> **Tip:** For very large datasets or multi-GPU/TPU training use Hugging Face’s `DistillationTrainer` or `accelerate`. The core algorithm below stays the same.\n",
    "\n",
    "---\n",
    "\n",
    "## Overview (what’s happening)\n",
    "1. Train a large **teacher** model (high capacity) normally and freeze it.  \n",
    "2. Train a smaller **student** model to mimic the teacher **and** the ground-truth labels.  \n",
    "3. Student loss = weighted combination of a **soft** (teacher) loss and a **hard** (label) loss.\n",
    "\n",
    "---\n",
    "\n",
    "## Step-by-step\n",
    "\n",
    "### 1. Teacher output (`t_soft`)\n",
    "- Compute teacher logits: `z_T = teacher(x)` (teacher is frozen; run under `torch.no_grad()`).\n",
    "- Apply **temperature** `T` and softmax to get *soft targets*:\n",
    "  \\[\n",
    "  p_T = \\text{softmax}\\!\\left(\\frac{z_T}{T}\\right)\n",
    "  \\]\n",
    "- `T > 1` “softens” the distribution (reveals class similarities).\n",
    "\n",
    "### 2. Student output (`s_soft`)\n",
    "- Compute student logits: `z_S = student(x)`.\n",
    "- Convert to log-probabilities at the same temperature:\n",
    "  \\[\n",
    "  \\log q_S = \\log\\text{softmax}\\!\\left(\\frac{z_S}{T}\\right)\n",
    "  \\]\n",
    "\n",
    "### 3. Distillation (soft) loss\n",
    "- Use KL divergence (teacher distribution → student distribution).\n",
    "- In PyTorch style: `nn.KLDivLoss(reduction='batchmean')(log_q_S, p_T)`\n",
    "- Multiply the KL loss by `T^2` to correct gradient scale (Hinton et al.):\n",
    "  \\[\n",
    "  L_{\\text{soft}} = T^2 \\cdot \\text{KL}(p_T \\,\\|\\, q_S)\n",
    "  \\]\n",
    "\n",
    "### 4. Hard (label) loss\n",
    "- Standard cross-entropy between student logits and true labels:\n",
    "  \\[\n",
    "  L_{\\text{hard}} = \\text{CE}(z_S, y)\n",
    "  \\]\n",
    "\n",
    "### 5. Combine\n",
    "- Weighted sum:\n",
    "  \\[\n",
    "  L = \\alpha \\cdot L_{\\text{soft}} + (1-\\alpha)\\cdot L_{\\text{hard}}\n",
    "  \\]\n",
    "- Typical choices: `T ∈ [2,5]`, `α ≈ 0.5` (tune for your task).\n",
    "- If `α = 1` → pure distillation (no hard labels). If `α = 0` → normal fine-tuning (no distillation).\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation notes / best practices\n",
    "- Freeze teacher: `teacher.eval()` and use `with torch.no_grad()` when generating `z_T`. This saves memory and avoids updating teacher weights.\n",
    "- Use `F.softmax(z_T / T, dim=-1)` for teacher targets and `F.log_softmax(z_S / T, dim=-1)` for student input to `KLDivLoss`.\n",
    "- In PyTorch, prefer `nn.KLDivLoss(reduction='batchmean')` for stable gradients.\n",
    "- Multiply KL term by `T**2` (important — otherwise gradients from soft targets are scaled down).\n",
    "- Monitor both components (`L_soft`, `L_hard`) and validation accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## Small numeric example (temperature effect)\n",
    "- Teacher logits: `[10, 2]`\n",
    "  - `T = 1` → `softmax([10,2]) ≈ [0.9997, 0.0003]` (very peaked)\n",
    "  - `T = 2` → `softmax([5,1]) ≈ [0.982, 0.018]` (softer, reveals second choice)\n",
    "  - `T = 5` → `softmax([2,0.4]) ≈ [0.83, 0.17]` (much softer)\n",
    "- Softer distributions reveal the teacher’s relative beliefs and help the student learn nuanced class relations.\n",
    "\n",
    "---\n",
    "\n",
    "## Why distillation helps\n",
    "- **Soft targets** encode “dark knowledge”: relative similarities between classes that hard labels hide.  \n",
    "- Student learns both the dataset labels **and** the teacher’s nuanced behavior → better generalization for a much smaller model.\n",
    "\n",
    "---\n",
    "\n",
    "## Short pseudocode (conceptual)\n",
    "1. `z_T = teacher(x)`  (no grad)\n",
    "2. `p_T = softmax(z_T / T)`\n",
    "3. `z_S = student(x)`\n",
    "4. `log_q_S = log_softmax(z_S / T)`\n",
    "5. `loss_soft = T^2 * KLDiv(log_q_S, p_T)`\n",
    "6. `loss_hard = CrossEntropy(z_S, y)`\n",
    "7. `loss = alpha * loss_soft + (1 - alpha) * loss_hard`\n",
    "8. `loss.backward()` and `optimizer.step()` (update only student)\n",
    "\n",
    "---\n",
    "\n",
    "## Quick hyperparameter suggestions\n",
    "- `T = 2` (good starting point), try `2–5`.  \n",
    "- `alpha = 0.3–0.7` depending on trust in teacher vs labels.  \n",
    "- `batch size`: 64–256 (task-dependent).  \n",
    "- Ensure teacher has good accuracy before distillation.\n",
    "\n",
    "---\n",
    "\n",
    "## Final note\n",
    "Distillation is **not** just fine-tuning: it explicitly transfers the teacher’s learned distributional knowledge (soft targets) into a compact student while still respecting hard labels. For large-scale runs, use `DistillationTrainer` / `accelerate` to scale cleanly across devices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7b6c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade datasets fsspec transformers"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
